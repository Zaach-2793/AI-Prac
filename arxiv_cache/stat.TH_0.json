"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Astat.TH%26id_list%3D%26start%3D0%26max_results%3D50\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=cat:stat.TH&amp;id_list=&amp;start=0&amp;max_results=50</title>\n  <id>http://arxiv.org/api/MBf6GzelNlGCD/K61HvSB331Efk</id>\n  <updated>2025-03-22T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">24649</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">50</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/math/0107135v2</id>\n    <updated>2002-06-16T12:48:34Z</updated>\n    <published>2001-07-19T13:40:32Z</published>\n    <title>Nonparametric Volatility Density Estimation</title>\n    <summary>  We consider two kinds of stochastic volatility models. Both kinds of models\ncontain a stationary volatility process, the density of which, at a fixed\ninstant in time, we aim to estimate.\n  We discuss discrete time models where for instance a log price process is\nmodeled as the product of a volatility process and i.i.d. noise. We also\nconsider samples of certain continuous time diffusion processes. The sampled\ntime instants will be be equidistant with vanishing distance.\n  A Fourier type deconvolution kernel density estimator based on the logarithm\nof the squared processes is proposed to estimate the volatility density.\nExpansions of the bias and bounds on the variances are derived.\n</summary>\n    <author>\n      <name>Bert van Es</name>\n    </author>\n    <author>\n      <name>Peter Spreij</name>\n    </author>\n    <author>\n      <name>Harry van Zanten</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.3150/bj/1065444813</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.3150/bj/1065444813\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Bernoulli 9 (3), 451-645 (2003)</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0107135v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0107135v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G07, 62M07, 62P20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0109002v1</id>\n    <updated>2001-09-01T00:33:15Z</updated>\n    <published>2001-09-01T00:33:15Z</published>\n    <title>Asymptotic accuracy of the jackknife variance estimator for certain\n  smooth statistics</title>\n    <summary>  We show that that the jackknife variance estimator $v_{jack}$ and the the\ninfinitesimal jackknife variance estimator are asymptotically equivalent if the\nfunctional of interest is a smooth function of the mean or a smooth trimmed\nL-statistic. We calculate the asymptotic variance of $v_{jack}$ for these\nfunctionals.\n</summary>\n    <author>\n      <name>Alex D Gottlieb</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0109002v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0109002v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G09; 62G20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0111152v1</id>\n    <updated>2001-11-13T15:34:58Z</updated>\n    <published>2001-11-13T15:34:58Z</published>\n    <title>Approximating distribution functions by iterated function systems</title>\n    <summary>  In this paper an iterated function system on the space of distribution\nfunctions is built. The inverse problem is introduced and studied by convex\noptimization problems. Some applications of this method to approximation of\ndistribution functions and to estimation theory are given.\n</summary>\n    <author>\n      <name>Stefano M. Iacus</name>\n    </author>\n    <author>\n      <name>Davide La Torre</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">1 table</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0111152v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0111152v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62E17; 62H10; 37H\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0111153v1</id>\n    <updated>2001-11-13T15:50:59Z</updated>\n    <published>2001-11-13T15:50:59Z</published>\n    <title>Statistical analysis of stochastic resonance with ergodic diffusion\n  noise</title>\n    <summary>  A subthreshold signal is transmitted through a channel and may be detected\nwhen some noise -- with known structure and proportional to some level -- is\nadded to the data. There is an optimal noise level, called stochastic\nresonance, that corresponds to the highest Fisher information in the problem of\nestimation of the signal. As noise we consider an ergodic diffusion process and\nthe asymptotic is considered as time goes to infinity. We propose consistent\nestimators of the subthreshold signal and we solve further a problem of\nhypotheses testing. We also discuss evidence of stochastic resonance for both\nestimation and hypotheses testing problems via examples.\n</summary>\n    <author>\n      <name>Stefano M. Iacus</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/math/0111153v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0111153v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"93E10; 62M99\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0112032v1</id>\n    <updated>2001-12-04T13:20:28Z</updated>\n    <published>2001-12-04T13:20:28Z</published>\n    <title>Asymptotic normality of kernel type deconvolution estimators</title>\n    <summary>  We derive asymptotic normality of kernel type deconvolution estimators of the\ndensity, the distribution function at a fixed point, and of the probability of\nan interval. We consider the so called super smooth case where the\ncharacteristic function of the known distribution decreases exponentially.\n  It turns out that the limit behavior of the pointwise estimators of the\ndensity and distribution function is relatively straightforward while the\nasymptotics of the estimator of the probability of an interval depends in a\ncomplicated way on the sequence of bandwidths.\n</summary>\n    <author>\n      <name>A. J. van Es</name>\n    </author>\n    <author>\n      <name>H. -W. Uh</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0112032v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0112032v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05; 62E20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0112298v1</id>\n    <updated>2001-12-28T17:09:40Z</updated>\n    <published>2001-12-28T17:09:40Z</published>\n    <title>Annuities under random rates of interest - revisited</title>\n    <summary>  In the article we consider accumulated values of annuities-certain with\nyearly payments with independent random interest rates. We focus on annuities\nwith payments varying in arithmetic and geometric progression which are\nimportant basic varying annuities (see Kellison, 1991). They appear to be a\ngeneralization of the types studied recently by Zaks (2001). We derive, via\nrecursive relationships, mean and variance formulae of the final values of the\nannuities. As a consequence, we obtain moments related to the already discussed\ncases, which leads to a correction of main results from Zaks (2001).\n</summary>\n    <author>\n      <name>K. Burnecki</name>\n    </author>\n    <author>\n      <name>A. Marciniuk</name>\n    </author>\n    <author>\n      <name>A. Weron</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Insurance: Mathematics &amp; Economics 32.3 (2003), 457-460</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0112298v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0112298v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"91B02; 62P05; 62P20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0202274v1</id>\n    <updated>2002-02-26T14:41:05Z</updated>\n    <published>2002-02-26T14:41:05Z</published>\n    <title>Estimation of Weibull Shape Parameter by Shrinkage Towards an Interval\n  Under Failure Censored Sampling</title>\n    <summary>  This paper is speculated to propose a class of shrinkage estimators for shape\nparameter beta in failure censored samples from two-parameter Weibull\ndistribution when some 'apriori' or guessed interval containing the parameter\nbeta is available in addition to sample information and analyses their\nproperties. Some estimators are generated from the proposed class and compared\nwith the minimum mean squared error (MMSE) estimator. Numerical computations in\nterms of percent relative efficiency and absolute relative bias indicate that\ncertain of these estimators substantially improve the MMSE estimator in some\nguessed interval of the parameter space of beta, especially for censored\nsamples with small sizes. Subsequently, a modified class of shrinkage\nestimators is proposed with its properties.\n</summary>\n    <author>\n      <name>Housila P. Singh</name>\n    </author>\n    <author>\n      <name>Sharad Saxena</name>\n    </author>\n    <author>\n      <name>Jack Allen</name>\n    </author>\n    <author>\n      <name>Sarjinder Singh</name>\n    </author>\n    <author>\n      <name>Florentin Smarandache</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">20 pages, 2 tables, 1 figure</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0202274v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0202274v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62E17\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0203080v2</id>\n    <updated>2002-03-11T09:18:41Z</updated>\n    <published>2002-03-08T14:34:36Z</published>\n    <title>Estimating a structural distribution function by grouping</title>\n    <summary>  By the method of Poissonization we confirm some existing results concerning\nconsistent estimation of the structural distribution function in the situation\nof a large number of rare events. Inconsistency of the so called natural\nestimator is proved. The method of grouping in cells of equal size is\ninvestigated and its consistency derived. A bound on the mean squared error is\nderived.\n</summary>\n    <author>\n      <name>Bert van Es</name>\n    </author>\n    <author>\n      <name>Stamatis Kolios</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/math/0203080v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0203080v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05; 62G20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0206006v2</id>\n    <updated>2002-08-03T23:05:22Z</updated>\n    <published>2002-06-03T01:46:38Z</published>\n    <title>An Illuminating Counterexample</title>\n    <summary>  We give a visually appealing counterexample to the proposition that unbiased\nestimators are better than biased estimators.\n</summary>\n    <author>\n      <name>Michael Hardy</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">6 pages, LaTeX, To appear in the American Mathematical Monthly</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0206006v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0206006v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62F10, 62F15\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0206142v1</id>\n    <updated>2002-06-14T10:42:03Z</updated>\n    <published>2002-06-14T10:42:03Z</published>\n    <title>Nonparametric volatility density estimation for discrete time models</title>\n    <summary>  We consider discrete time models for asset prices with a stationary\nvolatility process. We aim at estimating the multivariate density of this\nprocess at a set of consecutive time instants. A Fourier type deconvolution\nkernel density estimator based on the logarithm of the squared process is\nproposed to estimate the volatility density. Expansions of the bias and bounds\non the variance are derived.\n</summary>\n    <author>\n      <name>Bert van Es</name>\n    </author>\n    <author>\n      <name>Peter Spreij</name>\n    </author>\n    <author>\n      <name>Harry van Zanten</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1080/1048525042000267752</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1080/1048525042000267752\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Nonparametric Statistics 17 (2), 237-249 (2005)</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0206142v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0206142v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G07, 62M07, 62P20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0207044v1</id>\n    <updated>2002-07-04T11:25:50Z</updated>\n    <published>2002-07-04T11:25:50Z</published>\n    <title>On-line tracking of a smooth regression function</title>\n    <summary>  We construct an on-line estimator with equidistant design for tracking a\nsmooth function from Stone-Ibragimov-Khasminskii class. This estimator has the\noptimal convergence rate of risk to zero in sample size. The procedure for\nsetting coefficients of the estimator is controlled by a single parameter and\nhas a simple numerical solution. The off-line version of this estimator allows\nto eliminate a boundary layer. Simulation results are given.\n</summary>\n    <author>\n      <name>L. Goldentayer</name>\n    </author>\n    <author>\n      <name>R. Liptser</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 2 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0207044v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0207044v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05; Secondary 62M99\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0210425v1</id>\n    <updated>2002-10-28T09:40:55Z</updated>\n    <published>2002-10-28T09:40:55Z</published>\n    <title>Estimating the structural distribution function of cell probabilities</title>\n    <summary>  We consider estimation of the structural distribution function of the cell\nprobabilities of a multinomial sample in situations where the number of cells\nis large. We review the performance of the natural estimator, an estimator\nbased on grouping the cells and a kernel type estimator. Inconsistency of the\nnatural estimator and weak consistency of the other two estimators is derived\nby Poissonization and other, new, technical devices.\n</summary>\n    <author>\n      <name>B. van Es</name>\n    </author>\n    <author>\n      <name>C. A. J. Klaassen</name>\n    </author>\n    <author>\n      <name>R. M. Mnatsakanov</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/math/0210425v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0210425v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05; 62G20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0211079v3</id>\n    <updated>2011-01-05T10:39:54Z</updated>\n    <published>2002-11-05T10:29:01Z</published>\n    <title>Combining kernel estimators in the uniform deconvolution problem</title>\n    <summary>  We construct a density estimator and an estimator of the distribution\nfunction in the uniform deconvolution model. The estimators are based on\ninversion formulas and kernel estimators of the density of the observations and\nits derivative. Asymptotic normality and the asymptotic biases are derived.\n</summary>\n    <author>\n      <name>Bert van Es</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/math/0211079v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0211079v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05, 62E20, 62G07, 62G20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0212007v2</id>\n    <updated>2003-05-22T11:53:17Z</updated>\n    <published>2002-12-01T12:26:18Z</published>\n    <title>Asymptotic Normality of Nonparametric Kernel Type Deconvolution Density\n  Estimators: crossing the Cauchy boundary</title>\n    <summary>  We derive asymptotic normality of kernel type deconvolution density\nestimators. In particular we consider deconvolution problems where the known\ncomponent of the convolution has a symmetric lambda-stable distribution,\n0&lt;lambda&lt;= 2. It turns out that the limit behavior changes if the exponent\nparameter lambda passes the value one, the case of Cauchy deconvolution.\n</summary>\n    <author>\n      <name>A. J. van Es</name>\n    </author>\n    <author>\n      <name>H. -W. Uh</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/math/0212007v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0212007v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05; 62E20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0212350v1</id>\n    <updated>2002-12-27T13:33:17Z</updated>\n    <published>2002-12-27T13:33:17Z</published>\n    <title>Asymptotically efficient estimation of linear functionals in inverse\n  regression models</title>\n    <summary>  In this paper we will discuss a procedure to improve the usual estimator of a\nlinear functional of the unknown regression function in inverse nonparametric\nregression models. In Klaassen, Lee, and Ruymgaart (2001) it has been proved\nthat this traditional estimator is not asymptotically efficient (in the sense\nof the H\\'{a}jek - Le Cam convolution theorem) except, possibly, when the error\ndistribution is normal. Since this estimator, however, is still root-n\nconsistent a procedure in Bickel, Klaassen, Ritov, and Wellner (1993) applies\nto construct a modification which is asymptotically efficient. A self-contained\nproof of the asymptotic efficiency is included.\n</summary>\n    <author>\n      <name>Chris A. J. Klaassen</name>\n    </author>\n    <author>\n      <name>Eun-Joo Lee</name>\n    </author>\n    <author>\n      <name>Frits H. Ruymgaart</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0212350v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0212350v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G08; 62G20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0212395v1</id>\n    <updated>2002-12-01T00:00:00Z</updated>\n    <published>2002-12-01T00:00:00Z</published>\n    <title>Emerging applications of geometric multiscale analysis</title>\n    <summary>  Classical multiscale analysis based on wavelets has a number of successful\napplications, e.g. in data compression, fast algorithms, and noise removal.\nWavelets, however, are adapted to point singularities, and many phenomena in\nseveral variables exhibit intermediate-dimensional singularities, such as\nedges, filaments, and sheets. This suggests that in higher dimensions, wavelets\nought to be replaced in certain applications by multiscale analysis adapted to\nintermediate-dimensional singularities.\n  My lecture described various initial attempts in this direction. In\nparticular, I discussed two approaches to geometric multiscale analysis\noriginally arising in the work of Harmonic Analysts Hart Smith and Peter Jones\n(and others): (a) a directional wavelet transform based on parabolic dilations;\nand (b) analysis via anistropic strips. Perhaps surprisingly, these tools have\npotential applications in data compression, inverse problems, noise removal,\nand signal detection; applied mathematicians, statisticians, and engineers are\neagerly pursuing these leads.\n</summary>\n    <author>\n      <name>David L. Donoho</name>\n    </author>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the ICM, Beijing 2002, vol. 1, 209--233</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0212395v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0212395v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"41A30, 41A58, 41A63, 62G07, 62G08, 94A08, 94A11, 94A12, 94A29\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0212410v1</id>\n    <updated>2002-12-01T00:00:00Z</updated>\n    <published>2002-12-01T00:00:00Z</published>\n    <title>Hidden Markov and state space models: asymptotic analysis of exact and\n  approximate methods for prediction, filtering, smoothing and statistical\n  inference</title>\n    <summary>  State space models have long played an important role in signal processing.\nThe Gaussian case can be treated algorithmically using the famous Kalman\nfilter. Similarly since the 1970s there has been extensive application of\nHidden Markov models in speech recognition with prediction being the most\nimportant goal. The basic theoretical work here, in the case $X$ and $Y$ finite\n(small) providing both algorithms and asymptotic analysis for inference is that\nof Baum and colleagues. During the last 30-40 years these general models have\nproved of great value in applications ranging from genomics to finance.\n  Unless the $X,Y$ are jointly Gaussian or $X$ is finite and small the problem\nof calculating the distributions discussed and the likelihood exactly are\nnumerically intractable and if $Y$ is not finite asymptotic analysis becomes\nmuch more difficult. Some new developments have been the construction of\nso-called ``particle filters'' (Monte Carlo type) methods for approximate\ncalculation of these distributions (see Doucet et al. [4]) for instance and\ngeneral asymptotic methods for analysis of statistical methods in HMM [2] and\nother authors.\n  We will discuss these methods and results in the light of exponential mixing\nproperties of the conditional (posterior) distribution of $(X_1,X_2,...)$ given\n$(Y_1,Y_2,...)$ already noted by Baum and Petrie and recent work of the authors\nBickel, Ritov and Ryden, Del Moral and Jacod, Douc and Matias.\n</summary>\n    <author>\n      <name>Peter Bickel</name>\n    </author>\n    <author>\n      <name>Yaacov Ritov</name>\n    </author>\n    <author>\n      <name>Tobias Ryd\u00e9n</name>\n    </author>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the ICM, Beijing 2002, vol. 1, 555--556</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0212410v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0212410v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"60, 62\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0212411v1</id>\n    <updated>2002-12-01T00:00:00Z</updated>\n    <published>2002-12-01T00:00:00Z</published>\n    <title>Statistical equivalence and stochastic process limit theorems</title>\n    <summary>  A classical limit theorem of stochastic process theory concerns the sample\ncumulative distribution function (CDF) from independent random variables. If\nthe variables are uniformly distributed then these centered CDFs converge in a\nsuitable sense to the sample paths of a Brownian Bridge. The so-called\nHungarian construction of Komlos, Major and Tusnady provides a strong form of\nthis result. In this construction the CDFs and the Brownian Bridge sample paths\nare coupled through an appropriate representation of each on the same\nmeasurable space, and the convergence is uniform at a suitable rate.\n  Within the last decade several asymptotic statistical-equivalence theorems\nfor nonparametric problems have been proven, beginning with Brown and Low\n(1996) and Nussbaum (1996). The approach here to statistical-equivalence is\nfirmly rooted within the asymptotic statistical theory created by L. Le Cam but\nin some respects goes beyond earlier results.\n  This talk demonstrates the analogy between these results and those from the\ncoupling method for proving stochastic process limit theorems. These two\nclasses of theorems possess a strong inter-relationship, and technical methods\nfrom each domain can profitably be employed in the other. Results in a recent\npaper by Carter, Low, Zhang and myself will be described from this perspective.\n</summary>\n    <author>\n      <name>Lawrence D. Brown</name>\n    </author>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the ICM, Beijing 2002, vol. 1, 557--566</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0212411v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0212411v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0301363v1</id>\n    <updated>2003-01-31T05:45:12Z</updated>\n    <published>2003-01-31T05:45:12Z</published>\n    <title>Asymptotic equivalence of the jackknife and infinitesimal jackknife\n  variance estimators for some smooth statistics</title>\n    <summary>  The jackknife variance estimator and the the infinitesimal jackknife variance\nestimator are shown to be asymptotically equivalent if the functional of\ninterest is a smooth function of the mean or a trimmed L-statistic with Hoelder\ncontinuous weight function.\n</summary>\n    <author>\n      <name>Alex D. Gottlieb</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages. To appear in Annals of the Institute of Statistical\n  Mathematics</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0301363v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0301363v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G09; 62G20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0302079v1</id>\n    <updated>2003-02-07T17:27:42Z</updated>\n    <published>2003-02-07T17:27:42Z</published>\n    <title>Selection Criterion for Log-Linear Models Using Statistical Learning\n  Theory</title>\n    <summary>  Log-linear models are a well-established method for describing statistical\ndependencies among a set of n random variables. The observed frequencies of the\nn-tuples are explained by a joint probability such that its logarithm is a sum\nof functions, where each function depends on as few variables as possible. We\nobtain for this class a new model selection criterion using nonasymptotic\nconcepts of statistical learning theory. We calculate the VC dimension for the\nclass of k-factor log-linear models. In this way we are not only able to select\nthe model with the appropriate complexity, but obtain also statements on the\nreliability of the estimated probability distribution. Furthermore we show that\nthe selection of the best model among a set of models with the same complexity\ncan be written as a convex optimization problem.\n</summary>\n    <author>\n      <name>Daniel Herrmann</name>\n    </author>\n    <author>\n      <name>Dominik Janzing</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages, no figure, latex</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0302079v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0302079v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62H17, 62H15, 62H12 (Primary) 62G10, 62G07, 62G15 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0305234v1</id>\n    <updated>2003-05-16T08:33:22Z</updated>\n    <published>2003-05-16T08:33:22Z</published>\n    <title>Efficient estimation in the accelerated failure time model under cross\n  sectional sampling</title>\n    <summary>  Consider estimation of the regression parameter in the accelerated failure\ntime model, when data are obtained by cross sectional sampling. It is shown\nthat it is possible under regularity of the model to construct an efficient\nestimator of the unknown Euclidean regression parameter if the distribution of\nthe covariate vector is known and also if it is unknown with vanishing mean.\n</summary>\n    <author>\n      <name>Chris A. J. Klaassen</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">KdV-Institute, University of Amsterdam</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Philip J. Mokveld</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">KdV-Institute, University of Amsterdam</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Bert van Es</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">KdV-Institute, University of Amsterdam</arxiv:affiliation>\n    </author>\n    <link href=\"http://arxiv.org/abs/math/0305234v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0305234v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62N02 (primary), 62G05, 62N99 (secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0305273v2</id>\n    <updated>2004-02-16T16:11:23Z</updated>\n    <published>2003-05-19T18:06:40Z</published>\n    <title>Parametric Estimation of Diffusion Processes Sampled at First Exit Times</title>\n    <summary>  This paper introduces a family of recursively defined estimators of the\nparameters of a diffusion process. We use ideas of stochastic algorithms for\nthe construction of the estimators. Asymptotic consistency of these estimators\nand asymptotic normality of an appropriate normalization are proved. The\nresults are applied to two examples from the financial literature; viz.,\nCox-Ingersoll-Ross' model and the constant elasticity of variance (CEV) process\nillustrate the use of the technique proposed herein.\n</summary>\n    <author>\n      <name>Jaime A. Londo\u00f1o</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">31 pages, http://math.ucr.edu/~jlondono</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Journal of Pure and Applied Mathematics, 7, No. 4\n  (2003), 449-486. MR1994830 (2004c:62175)</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0305273v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0305273v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62M05, 62P05\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0306237v1</id>\n    <updated>2003-06-16T11:24:56Z</updated>\n    <published>2003-06-16T11:24:56Z</published>\n    <title>Rates of convergence for constrained deconvolution problem</title>\n    <summary>  Let $X$ and $Y$ be two independent identically distributed random variables\nwith density $p(x)$ and $Z=\\alpha X+\\beta Y$ for some constants $\\alpha&gt;0$ and\n$\\beta&gt;0$. We consider the problem of estimating $p(x)$ by means of the samples\nfrom the distribution of $Z$. Non-parametric estimator based on the sync kernel\nis constructed and asymptotic behaviour of the corresponding mean integrated\nsquare error is investigated.\n</summary>\n    <author>\n      <name>Denis Belomestny</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/math/0306237v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0306237v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05; 62E20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0309355v1</id>\n    <updated>2003-09-22T08:36:06Z</updated>\n    <published>2003-09-22T08:36:06Z</published>\n    <title>On the largest eigenvalue of Wishart matrices with identity covariance\n  when n, p and p/n tend to infinity</title>\n    <summary>  Let X be a n*p matrix and l_1 the largest eigenvalue of the covariance matrix\nX^{*}*X. The \"null case\" where X_{i,j} are independent Normal(0,1) is of\nparticular interest for principal component analysis. For this model, when n, p\ntend to infinity and n/p tends to gamma in (0,\\infty), it was shown in\nJohnstone (2001) that l_1, properly centered and scaled, converges to the\nTracy-Widom law. We show that with the same centering and scaling, the result\nis true even when p/n or n/p tends to infinity. The derivation uses ideas and\ntechniques quite similar to the ones presented in Johnstone (2001). Following\nSoshnikov (2002), we also show that the same is true for the joint distribution\nof the k largest eigenvalues, where k is a fixed integer. Numerical experiments\nillustrate the fact that the Tracy-Widom approximation is reasonable even when\none of the dimension is \"small\".\n</summary>\n    <author>\n      <name>Noureddine El Karoui</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">24 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0309355v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0309355v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62E20 (Primary) 62H25 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0310006v3</id>\n    <updated>2003-10-09T23:14:00Z</updated>\n    <published>2003-10-01T15:17:06Z</published>\n    <title>The marginalization paradox does not imply inconsistency for improper\n  priors</title>\n    <summary>  The marginalization paradox involves a disagreement between two Bayesians who\nuse two different procedures for calculating a posterior in the presence of an\nimproper prior. We show that the argument used to justify the procedure of one\nof the Bayesians is inapplicable. There is therefore no reason to expect\nagreement, no paradox, and no evidence that improper priors are inherently\ninconsistent. We show further that the procedure in question can be interpreted\nas the cancellation of infinities in the formal posterior. We suggest that the\nimplicit use of this formal procedure is the source of the observed\ndisagreement.\n</summary>\n    <author>\n      <name>Timothy C. Wallstrom</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">5 pages. Minor change to discussion section. Comments welcome</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0310006v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0310006v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62A01\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0312056v1</id>\n    <updated>2003-12-02T10:43:32Z</updated>\n    <published>2003-12-02T10:43:32Z</published>\n    <title>Nonparametric Estimation in the Model of Moving Average</title>\n    <summary>  The subject of robust estimation in time series is widely discussed in\nliterature. One of the approaches is to use GM-estimation. This method\nincorporates a broad class of nonparametric estimators which under suitable\nconditions includes estimators robust to outliers in data. For the linear\nmodels the sensitivity of GM-estimators to outliers have been studied in the\nwork by Martin and Yohai [5], and influence functionals for this estimator were\nderived. In this paper we follow this direction and examine the asymptotical\nproperties of the class of M-estimators, which is narrower than the class of\nGM-estimators, but gives more insight into asymptotical properties of such\nestimators. This paper gives an asymptotic expansion of the residual weighted\nempirical process, which allows to prove asymptotic normality of these\nestimators in case of non-smooth objective functions. For simplicity MA(1)\nmodel is considered, but it will be shown that even in this case mathematical\ntechniques used to derive these asymptotic properties appear to be rather\ncomplicated.However, the approach used in this paper could be applied to\nGM-estimators and to more realistic models.\n</summary>\n    <author>\n      <name>Alexander Alekseev</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0312056v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0312056v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62P05; 62P20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0403373v1</id>\n    <updated>2004-03-22T21:29:09Z</updated>\n    <published>2004-03-22T21:29:09Z</published>\n    <title>Grade of Membership Analysis: One Possible Approach to Foundations</title>\n    <summary>  Grade of membership (GoM) analysis was introduced in 1974 as a means of\nanalyzing multivariate categorical data. Since then, it has been successfully\napplied to many problems. The primary goal of GoM analysis is to derive\nproperties of individuals based on results of multivariate measurements; such\nproperties are given in the form of the expectations of a hidden random\nvariable (state of an individual) conditional on the result of observations.\n  In this article, we present a new perspective for the GoM model, based on\nconsidering distribution laws of observed random variables as realizations of\nanother random variable. It happens that some moments of this new random\nvariable are directly estimable from observations. Our approach allows us to\nestablish a number of important relations between estimable moments and values\nof interest, which, in turn, provides a basis for a new numerical procedure.\n</summary>\n    <author>\n      <name>Mikhail Kovtun</name>\n    </author>\n    <author>\n      <name>Igor Akushevich</name>\n    </author>\n    <author>\n      <name>Kenneth G. Manton</name>\n    </author>\n    <author>\n      <name>H. Dennis Tolley</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0403373v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0403373v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62H12; 62J99\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0405511v1</id>\n    <updated>2004-05-26T21:39:53Z</updated>\n    <published>2004-05-26T21:39:53Z</published>\n    <title>The suppport reduction algorithm for computing nonparametric function\n  estimates in mixture models</title>\n    <summary>  Vertex direction algorithms have been around for a few decades in the\nexperimental design and mixture models literature. We briefly review this type\nof algorithm and describe a new member of the family: the support reduction\nalgorithm. The support reduction algorithm is applied to the problem of\ncomputing nonparametric estimates in two inverse problems: convex density\nestimation and the Gaussian deconvolution problem. Usually, VD algorithms solve\na finite dimensional (version of the) optimization problem of interest. We\nintroduce a method to solve the true infinite dimensional optimization problem.\n</summary>\n    <author>\n      <name>Piet Groeneboom</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Delft University</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Geurt Jongbloed</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Vrije University</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Jon A. Wellner</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">University of Washington</arxiv:affiliation>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages. See also http://www.cs.vu.nl/sto/publications/2002-13.ps</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/math/0405511v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0405511v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"65C (primary), 60 (secondary); 62-04\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406424v1</id>\n    <updated>2004-06-22T10:05:22Z</updated>\n    <published>2004-06-22T10:05:22Z</published>\n    <title>Multiscale likelihood analysis and complexity penalized estimation</title>\n    <summary>  We describe here a framework for a certain class of multiscale likelihood\nfactorizations wherein, in analogy to a wavelet decomposition of an L^2\nfunction, a given likelihood function has an alternative representation as a\nproduct of conditional densities reflecting information in both the data and\nthe parameter vector localized in position and scale. The framework is\ndeveloped as a set of sufficient conditions for the existence of such\nfactorizations, formulated in analogy to those underlying a standard\nmultiresolution analysis for wavelets, and hence can be viewed as a\nmultiresolution analysis for likelihoods. We then consider the use of these\nfactorizations in the task of nonparametric, complexity penalized likelihood\nestimation. We study the risk properties of certain thresholding and\npartitioning estimators, and demonstrate their adaptivity and near-optimality,\nin a minimax sense over a broad range of function spaces, based on squared\nHellinger distance as a loss function. In particular, our results provide an\nillustration of how properties of classical wavelet-based estimators can be\nobtained in a single, unified framework that includes models for continuous,\ncount and categorical data types.\n</summary>\n    <author>\n      <name>Eric D. Kolaczyk</name>\n    </author>\n    <author>\n      <name>Robert D. Nowak</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000076</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000076\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 500-527</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406424v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406424v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62C20, 62G05 (Primary) 60E05 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406425v1</id>\n    <updated>2004-06-22T10:09:23Z</updated>\n    <published>2004-06-22T10:09:23Z</published>\n    <title>Confidence balls in Gaussian regression</title>\n    <summary>  Starting from the observation of an R^n-Gaussian vector of mean f and\ncovariance matrix \\sigma^2 I_n (I_n is the identity matrix), we propose a\nmethod for building a Euclidean confidence ball around f, with prescribed\nprobability of coverage. For each n, we describe its nonasymptotic property and\nshow its optimality with respect to some criteria.\n</summary>\n    <author>\n      <name>Yannick Baraud</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000085</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000085\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 528-551</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406425v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406425v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G15 (Primary) 62G05, 62G10. (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406427v1</id>\n    <updated>2004-06-22T11:04:00Z</updated>\n    <published>2004-06-22T11:04:00Z</published>\n    <title>Minimax estimation of linear functionals over nonconvex parameter spaces</title>\n    <summary>  The minimax theory for estimating linear functionals is extended to the case\nof a finite union of convex parameter spaces. Upper and lower bounds for the\nminimax risk can still be described in terms of a modulus of continuity.\n  However in contrast to the theory for convex parameter spaces rate optimal\nprocedures are often required to be nonlinear. A construction of such nonlinear\nprocedures is given. The results developed in this paper have important\napplications to the theory of adaptation.\n</summary>\n    <author>\n      <name>T. Tony Cai</name>\n    </author>\n    <author>\n      <name>Mark G. Low</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000094</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000094\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 552-576</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406427v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406427v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G99 (Primary) 62F12, 62C20, 62M99. (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406430v1</id>\n    <updated>2004-06-22T12:59:41Z</updated>\n    <published>2004-06-22T12:59:41Z</published>\n    <title>Statistical inference for time-inhomogeneous volatility models</title>\n    <summary>  This paper offers a new approach for estimating and forecasting the\nvolatility of financial time series. No assumption is made about the parametric\nform of the processes. On the contrary, we only suppose that the volatility can\nbe approximated by a constant over some interval. In such a framework, the main\nproblem consists of filtering this interval of time homogeneity; then the\nestimate of the volatility can be simply obtained by local averaging.\n  We construct a locally adaptive volatility estimate (LAVE) which can perform\nthis task and investigate it both from the theoretical point of view and\nthrough Monte Carlo simulations. Finally, the LAVE procedure is applied to a\ndata set of nine exchange rates and a comparison with a standard GARCH model is\nalso provided. Both models appear to be capable of explaining many of the\nfeatures of the data; nevertheless, the new approach seems to be superior to\nthe GARCH method as far as the out-of-sample results are concerned.\n</summary>\n    <author>\n      <name>Danilo Mercurio</name>\n    </author>\n    <author>\n      <name>Vladimir Spokoiny</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000102</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000102\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 577-602</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406430v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406430v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62M10 (Primary) 62P20 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406431v1</id>\n    <updated>2004-06-22T13:02:37Z</updated>\n    <published>2004-06-22T13:02:37Z</published>\n    <title>Estimating invariant laws of linear processes by U-statistics</title>\n    <summary>  Suppose we observe an invertible linear process with independent mean-zero\ninnovations and with coefficients depending on a finite-dimensional parameter,\nand we want to estimate the expectation of some function under the stationary\ndistribution of the process. The usual estimator would be the empirical\nestimator. It can be improved using the fact that the innovations are centered.\n  We construct an even better estimator using the representation of the\nobservations as infinite-order moving averages of the innovations. Then the\nexpectation of the function under the stationary distribution can be written as\nthe expectation under the distribution of an infinite series in terms of the\ninnovations, and it can be estimated by a U-statistic of increasing order\n  (also called an ``infinite-order U-statistic'') in terms of the estimated\ninnovations. The estimator can be further improved using the fact that the\ninnovations are centered. This improved estimator is optimal if the\ncoefficients of the linear process are estimated optimally.\n</summary>\n    <author>\n      <name>Anton Schick</name>\n    </author>\n    <author>\n      <name>Wolfgang Wefelmeyer</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000111</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000111\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 603-632</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406431v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406431v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62M09, 62M10 (Primary) 62G05, 62G20. (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406432v1</id>\n    <updated>2004-06-22T13:04:22Z</updated>\n    <published>2004-06-22T13:04:22Z</published>\n    <title>The efficiency of the estimators of the parameters in GARCH processes</title>\n    <summary>  We propose a class of estimators for the parameters of a GARCH(p,q) sequence.\n  We show that our estimators are consistent and asymptotically normal under\nmild conditions. The quasi-maximum likelihood and the likelihood estimators are\ndiscussed in detail. We show that the maximum likelihood estimator is optimal.\nIf the tail of the distribution of the innovations is polynomial, even a\nquasi-maximum likelihood estimator based on exponential density performs better\nthan the standard normal density-based quasi-likelihood estimator of Lee and\nHansen and Lumsdaine.\n</summary>\n    <author>\n      <name>Istv\u00e1n Berkes</name>\n    </author>\n    <author>\n      <name>Lajos Horv\u00e1th</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000120</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000120\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 633-655</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406432v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406432v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62F12 (Primary) 62M10. (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406433v1</id>\n    <updated>2004-06-22T13:25:31Z</updated>\n    <published>2004-06-22T13:25:31Z</published>\n    <title>Selecting optimal multistep predictors for autoregressive processes of\n  unknown order</title>\n    <summary>  We consider the problem of choosing the optimal (in the sense of mean-squared\nprediction error) multistep predictor for an autoregressive (AR) process of\nfinite but unknown order. If a working AR model (which is possibly\nmisspecified) is adopted for multistep predictions, then two competing types of\nmultistep predictors (i.e., plug-in and direct predictors) can be obtained from\nthis model. We provide some interesting examples to show that when both plug-in\nand direct predictors are considered, the optimal multistep prediction results\ncannot be guaranteed by correctly identifying the underlying model's order.\nThis finding challenges the traditional model (order) selection criteria, which\nusually aim to choose the order of the true model. A new prediction selection\ncriterion, which attempts to seek the best combination of the prediction order\nand the prediction method, is proposed to rectify this difficulty. When the\nunderlying model is stationary, the validity of the proposed criterion is\njustified theoretically.\n</summary>\n    <author>\n      <name>Ching-Kang Ing</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000148</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000148\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 693-722</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406433v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406433v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62M20 (Primary) 62M10, 60F15. (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406451v1</id>\n    <updated>2004-06-23T07:46:38Z</updated>\n    <published>2004-06-23T07:46:38Z</published>\n    <title>Missing at random, likelihood ignorability and model completeness</title>\n    <summary>  This paper provides further insight into the key concept of missing at random\n(MAR) in incomplete data analysis. Following the usual selection modelling\napproach we envisage two models with separable parameters: a model for the\nresponse of interest and a model for the missing data mechanism\n  (MDM). If the response model is given by a complete density family, then\nfrequentist inference from the likelihood function ignoring the MDM is valid if\nand only if the MDM is MAR. This necessary and sufficient condition also holds\nmore generally for models for coarse data, such as censoring.\n  Examples are given to show the necessity of the completeness of the\nunderlying model for this equivalence to hold.\n</summary>\n    <author>\n      <name>Guobing Lu</name>\n    </author>\n    <author>\n      <name>John B. Copas</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000166</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000166\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 754-765</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406451v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406451v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62B99, 62F10, 62N01 (Primary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406452v1</id>\n    <updated>2004-06-23T07:56:36Z</updated>\n    <published>2004-06-23T07:56:36Z</published>\n    <title>Information bounds for Cox regression models with missing data</title>\n    <summary>  We derive information bounds for the regression parameters in Cox models when\ndata are missing at random. These calculations are of interest for\nunderstanding the behavior of efficient estimation in case-cohort designs, a\ntype of two-phase design often used in cohort studies. The derivations make use\nof key lemmas appearing in Robins, Rotnitzky and Zhao [J. Amer. Statist. Assoc.\n89 (1994) 846-866] and Robins, Hsieh and Newey [J. Roy. Statist. Soc. Ser. B 57\n(1995) 409-424], but in a form suited for our purposes here. We begin by\nsummarizing the results of Robins, Rotnitzky and Zhao in a form that leads\ndirectly to the projection method which will be of use for our model of\ninterest. We then proceed to derive new information bounds for the regression\nparameters of the Cox model with data Missing At Random (MAR). In the final\nsection we exemplify our calculations with several models of interest in cohort\nstudies, including an i.i.d. version of the classical case-cohort design of\nPrentice [Biometrika 73 (1986) 1-11]\n</summary>\n    <author>\n      <name>Bin Nan</name>\n    </author>\n    <author>\n      <name>Mary J. Emond</name>\n    </author>\n    <author>\n      <name>Jon A. Wellner</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000157</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000157\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 723-753</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406452v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406452v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62E17 (Primary) 65D20 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406453v1</id>\n    <updated>2004-06-23T07:59:03Z</updated>\n    <published>2004-06-23T07:59:03Z</published>\n    <title>Finite sample properties of multiple imputation estimators</title>\n    <summary>  Finite sample properties of multiple imputation estimators under the linear\nregression model are studied. The exact bias of the multiple imputation\nvariance estimator is presented. A method of reducing the bias is presented and\nsimulation is used to make comparisons. We also show that the suggested method\ncan be used for a general class of linear estimators.\n</summary>\n    <author>\n      <name>Jae Kwang Kim</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000175</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000175\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 766-783</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406453v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406453v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62D05 (Primary) 62J99 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406454v1</id>\n    <updated>2004-06-23T08:01:45Z</updated>\n    <published>2004-06-23T08:01:45Z</published>\n    <title>Sufficient burn-in for Gibbs samplers for a hierarchical random effects\n  model</title>\n    <summary>  We consider Gibbs and block Gibbs samplers for a Bayesian hierarchical\nversion of the one-way random effects model. Drift and minorization conditions\nare established for the underlying Markov chains. The drift and minorization\nare used in conjunction with results from J. S. Rosenthal [J. Amer. Statist.\n  Assoc. 90 (1995) 558-566] and G. O. Roberts and R. L. Tweedie [Stochastic\n  Process. Appl. 80 (1999) 211-229] to construct analytical upper bounds on the\ndistance to stationarity. These lead to upper bounds on the amount of burn-in\nthat is required to get the chain within a prespecified (total variation)\ndistance of the stationary distribution. The results are illustrated with a\nnumerical example.\n</summary>\n    <author>\n      <name>Galin L. Jones</name>\n    </author>\n    <author>\n      <name>James P. Hobert</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000184</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000184\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 784-817</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406454v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406454v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"60J10 (Primary) 62F15 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406455v1</id>\n    <updated>2004-06-23T08:03:59Z</updated>\n    <published>2004-06-23T08:03:59Z</published>\n    <title>Mean squared error of empirical predictor</title>\n    <summary>  The term ``empirical predictor'' refers to a two-stage predictor of a linear\ncombination of fixed and random effects. In the first stage, a predictor is\nobtained but it involves unknown parameters; thus, in the second stage, the\nunknown parameters are replaced by their estimators. In this paper, we consider\nmean squared errors (MSE) of empirical predictors under a general setup, where\nML or REML estimators are used for the second stage. We obtain second-order\napproximation to the MSE as well as an estimator of the MSE correct to the same\norder. The general results are applied to mixed linear models to obtain a\nsecond-order approximation to the MSE of the empirical best linear unbiased\npredictor (EBLUP) of a linear mixed effect and an estimator of the MSE of EBLUP\nwhose bias is correct to second order. The general mixed linear model includes\nthe mixed ANOVA model and the longitudinal model as special cases.\n</summary>\n    <author>\n      <name>Kalyan Das</name>\n    </author>\n    <author>\n      <name>Jiming Jiang</name>\n    </author>\n    <author>\n      <name>J. N. K. Rao</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000201</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000201\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 818-840</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406455v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406455v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62F12, 62J99 (Primary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406456v2</id>\n    <updated>2004-06-30T07:51:21Z</updated>\n    <published>2004-06-23T08:06:25Z</published>\n    <title>Least Angle Regression</title>\n    <summary>  The purpose of model selection algorithms such as All Subsets, Forward\n  Selection and Backward Elimination is to choose a linear model on the basis\nof the same set of data to which the model will be applied. Typically we have\navailable a large collection of possible covariates from which we hope to\nselect a parsimonious set for the efficient prediction of a response variable.\nLeast Angle Regression (LARS), a new model selection algorithm, is a useful and\nless greedy version of traditional forward selection methods.\n  Three main properties are derived: (1) A simple modification of the LARS\nalgorithm implements the Lasso, an attractive version of ordinary least squares\nthat constrains the sum of the absolute regression coefficients; the LARS\nmodification calculates all possible Lasso estimates for a given problem, using\nan order of magnitude less computer time than previous methods.\n  (2) A different LARS modification efficiently implements Forward Stagewise\nlinear regression, another promising new model selection method;\n</summary>\n    <author>\n      <name>Bradley Efron</name>\n    </author>\n    <author>\n      <name>Trevor Hastie</name>\n    </author>\n    <author>\n      <name>Iain Johnstone</name>\n    </author>\n    <author>\n      <name>Robert Tibshirani</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000067</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000067\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper discussed in: math.ST/0406463, math.ST/0406467,\n  math.ST/0406468, math.ST/0406469, math.ST/0406470, math.ST/0406471,\n  math.ST/0406472, math.ST/0406473. Rejoinder in math.ST/0406474</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 407-451</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406456v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406456v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62J07. (Primary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406460v1</id>\n    <updated>2004-06-23T12:39:49Z</updated>\n    <published>2004-06-23T12:39:49Z</published>\n    <title>Training samples in objective Bayesian model selection</title>\n    <summary>  Central to several objective approaches to Bayesian model selection is the\nuse of training samples (subsets of the data), so as to allow utilization of\nimproper objective priors. The most common prescription for choosing training\nsamples is to choose them to be as small as possible, subject to yielding\nproper posteriors; these are called minimal training samples.\n  When data can vary widely in terms of either information content or impact on\nthe improper priors, use of minimal training samples can be inadequate.\n  Important examples include certain cases of discrete data, the presence of\ncensored observations, and certain situations involving linear models and\nexplanatory variables. Such situations require more sophisticated methods of\nchoosing training samples. A variety of such methods are developed in this\npaper, and successfully applied in challenging situations.\n</summary>\n    <author>\n      <name>James O. Berger</name>\n    </author>\n    <author>\n      <name>Luis R. Pericchi</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000229</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000229\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 3, 841-869</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406460v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406460v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62F03, 62F15 (Primary) 62N03, 62B10, 62F40. (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406462v1</id>\n    <updated>2004-06-23T11:58:45Z</updated>\n    <published>2004-06-23T11:58:45Z</published>\n    <title>Local Whittle estimation in nonstationary and unit root cases</title>\n    <summary>  Asymptotic properties of the local Whittle estimator in the nonstationary\ncase (d&gt;{1/2}) are explored. For {1/2}&lt;d\\leq 1, the estimator is shown to be\nconsistent, and its limit distribution and the rate of convergence depend on\nthe value of d. For d=1, the limit distribution is mixed normal.\n  For d&gt;1 and when the process has a polynomial trend of order \\alpha &gt;{1/2},\nthe estimator is shown to be inconsistent and to converge in probability to\nunity.\n</summary>\n    <author>\n      <name>Peter C. B. Phillips</name>\n    </author>\n    <author>\n      <name>Katsumi Shimotsu</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000139</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000139\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 656-692</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406462v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406462v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62M10. (Primary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406463v1</id>\n    <updated>2004-06-23T12:36:13Z</updated>\n    <published>2004-06-23T12:36:13Z</published>\n    <title>Discussion of \"Least angle regression\" by Efron et al</title>\n    <summary>  Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456]\n</summary>\n    <author>\n      <name>Hemant Ishwaran</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000067</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000067\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 452-458</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406463v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406463v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406464v1</id>\n    <updated>2004-06-23T12:41:22Z</updated>\n    <published>2004-06-23T12:41:22Z</published>\n    <title>Optimal predictive model selection</title>\n    <summary>  Often the goal of model selection is to choose a model for future prediction,\nand it is natural to measure the accuracy of a future prediction by squared\nerror loss. Under the Bayesian approach, it is commonly perceived that the\noptimal predictive model is the model with highest posterior probability, but\nthis is not necessarily the case. In this paper we show that, for selection\namong normal linear models, the optimal predictive model is often the median\nprobability model, which is defined as the model consisting of those variables\nwhich have overall posterior probability greater than or equal to 1/2 of being\nin a model. The median probability model often differs from the highest\nprobability model.\n</summary>\n    <author>\n      <name>Maria Maddalena Barbieri</name>\n    </author>\n    <author>\n      <name>James O. Berger</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000238</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000238\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 3, 870-897</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406464v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406464v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62F15 (Primary) 62C10. (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406465v1</id>\n    <updated>2004-06-23T13:03:07Z</updated>\n    <published>2004-06-23T13:03:07Z</published>\n    <title>Consistent covariate selection and post model selection inference in\n  semiparametric regression</title>\n    <summary>  This paper presents a model selection technique of estimation in\nsemiparametric regression models of the type\nY_i=\\beta^{\\prime}\\underbarX_i+f(T_i)+W_i, i=1,...,n. The parametric and\nnonparametric components are estimated simultaneously by this procedure.\nEstimation is based on a collection of finite-dimensional models, using a\npenalized least squares criterion for selection. We show that by tailoring the\npenalty terms developed for nonparametric regression to semiparametric models,\nwe can consistently estimate the subset of nonzero coefficients of the linear\npart. Moreover, the selected estimator of the linear component is\nasymptotically normal.\n</summary>\n    <author>\n      <name>Florentina Bunea</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000247</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000247\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 3, 898-927</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406465v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406465v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G05, 62F99 (Primary) 62G08, 62J02 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406466v1</id>\n    <updated>2004-06-23T12:46:22Z</updated>\n    <published>2004-06-23T12:46:22Z</published>\n    <title>Nonconcave penalized likelihood with a diverging number of parameters</title>\n    <summary>  A class of variable selection procedures for parametric models via nonconcave\npenalized likelihood was proposed by Fan and Li to simultaneously estimate\nparameters and select important variables. They demonstrated that this class of\nprocedures has an oracle property when the number of parameters is finite.\nHowever, in most model selection problems the number of parameters should be\nlarge and grow with the sample size. In this paper some asymptotic properties\nof the nonconcave penalized likelihood are established for situations in which\nthe number of parameters tends to \\infty as the sample size increases.\n  Under regularity conditions we have established an oracle property and the\nasymptotic normality of the penalized likelihood estimators. Furthermore, the\nconsistency of the sandwich formula of the covariance matrix is demonstrated.\n  Nonconcave penalized likelihood ratio statistics are discussed, and their\nasymptotic distributions under the null hypothesis are obtained by imposing\nsome mild conditions on the penalty functions.\n</summary>\n    <author>\n      <name>Jianqing Fan</name>\n    </author>\n    <author>\n      <name>Heng Peng</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000256</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000256\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 3, 928-961</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406466v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406466v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62F12 (Primary) 62J02, 62E20 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406467v1</id>\n    <updated>2004-06-23T12:49:22Z</updated>\n    <published>2004-06-23T12:49:22Z</published>\n    <title>Discussion of \"Least angle regression\" by Efron et al</title>\n    <summary>  Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456]\n</summary>\n    <author>\n      <name>Keith Knight</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000067</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000067\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 458-460</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406467v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406467v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406468v1</id>\n    <updated>2004-06-23T12:51:13Z</updated>\n    <published>2004-06-23T12:51:13Z</published>\n    <title>Discussion of \"Least angle regression\" by Efron et al</title>\n    <summary>  Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456]\n</summary>\n    <author>\n      <name>Jean-Michel Loubes</name>\n    </author>\n    <author>\n      <name>Pascal Massart</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000067</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000067\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 460-465</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406468v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406468v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/math/0406469v1</id>\n    <updated>2004-06-23T12:52:48Z</updated>\n    <published>2004-06-23T12:52:48Z</published>\n    <title>Discussion of \"Least angle regression\" by Efron et al</title>\n    <summary>  Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456]\n</summary>\n    <author>\n      <name>David Madigan</name>\n    </author>\n    <author>\n      <name>Greg Ridgeway</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/009053604000000067</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/009053604000000067\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2004, Vol. 32, No. 2, 465-469</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/math/0406469v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/math/0406469v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.ST\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.TH\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"