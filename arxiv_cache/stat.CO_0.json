"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Astat.CO%26id_list%3D%26start%3D0%26max_results%3D50\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=cat:stat.CO&amp;id_list=&amp;start=0&amp;max_results=50</title>\n  <id>http://arxiv.org/api/6O3cSCbIp1xEzdi+Jn68Ranhkpw</id>\n  <updated>2025-03-22T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">8949</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">50</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/0707.0167v1</id>\n    <updated>2007-07-02T08:39:27Z</updated>\n    <published>2007-07-02T08:39:27Z</published>\n    <title>The random Tukey depth</title>\n    <summary>  The computation of the Tukey depth, also called halfspace depth, is very\ndemanding, even in low dimensional spaces, because it requires the\nconsideration of all possible one-dimensional projections. In this paper we\npropose a random depth which approximates the Tukey depth. It only takes into\naccount a finite number of one-dimensional projections which are chosen at\nrandom. Thus, this random depth requires a very small computation time even in\nhigh dimensional spaces. Moreover, it is easily extended to cover the\nfunctional framework.\n  We present some simulations indicating how many projections should be\nconsidered depending on the sample size and on the dimension of the sample\nspace. We also compare this depth with some others proposed in the literature.\nIt is noteworthy that the random depth, based on a very low number of\nprojections, obtains results very similar to those obtained with other depths.\n</summary>\n    <author>\n      <name>J. A. Cuesta-Albertos</name>\n    </author>\n    <author>\n      <name>A. Nieto-Reyes</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0707.0167v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0707.0167v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0708.1051v1</id>\n    <updated>2007-08-08T07:15:05Z</updated>\n    <published>2007-08-08T07:15:05Z</published>\n    <title>Deconvolution by simulation</title>\n    <summary>  Given samples (x_1,...,x_m) and (z_1,...,z_n) which we believe are\nindependent realizations of random variables X and Z respectively, where we\nfurther believe that Z=X+Y with Y independent of X, the problem is to estimate\nthe distribution of Y. We present a new method for doing this, involving\nsimulation. Experiments suggest that the method provides useful estimates.\n</summary>\n    <author>\n      <name>Colin Mallows</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/074921707000000021</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/074921707000000021\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published at http://dx.doi.org/10.1214/074921707000000021 in the IMS\n  Lecture Notes Monograph Series\n  (http://www.imstat.org/publications/lecnotes.htm) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IMS Lecture Notes Monograph Series 2007, Vol. 54, 1-11</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0708.1051v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0708.1051v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"60J10, 62G05, 94C99 (Primary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0709.1721v1</id>\n    <updated>2007-09-11T20:43:15Z</updated>\n    <published>2007-09-11T20:43:15Z</published>\n    <title>Parallel marginalization Monte Carlo with applications to conditional\n  path sampling</title>\n    <summary>  Monte Carlo sampling methods often suffer from long correlation times.\nConsequently, these methods must be run for many steps to generate an\nindependent sample. In this paper a method is proposed to overcome this\ndifficulty. The method utilizes information from rapidly equilibrating coarse\nMarkov chains that sample marginal distributions of the full system. This is\naccomplished through exchanges between the full chain and the auxiliary coarse\nchains. Results of numerical tests on the bridge sampling and\nfiltering/smoothing problems for a stochastic differential equation are\npresented.\n</summary>\n    <author>\n      <name>Jonathan Weare</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0709.1721v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0709.1721v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0709.3560v7</id>\n    <updated>2008-11-07T18:57:05Z</updated>\n    <published>2007-09-24T04:55:59Z</published>\n    <title>On The Density Estimation by Super-Parametric Method</title>\n    <summary>  The super-parametric density estimators and its related algorism were\nsuggested by Y. -S. Tsai et al [7]. The number of parameters is unlimited in\nthe super- parametric estimators and it is a general theory in sense of\nunifying or connecting nonparametric and parametric estimators. Before applying\nto numerical examples, we can not give any comment of the estimators. In this\npaper, we will focus on the implementation, the computer programming, of the\nalgorism and strategies of choosing window functions. B-splines, Bezier splines\nand covering windows are studied as well. According to the criterion of the\nconvergence conditions for Parzen window, the number of the window functions\nshall be, roughly, proportional to the number of samples and so is the number\nof the variables. Since the algorism is designed for solving the optimization\nof likelihood function, there will be a set of nonlinear equations with a large\nnumber of variables. The results show that algorism suggested by Y. -S. Tsai is\nvery powerful and effective in the sense of mathematics, that is, the iteration\nprocedures converge and the rates of convergence are very fast. Also, the\nnumerical results of different window functions show that the approach of\nsuper-parametric density estimators has ushered a new era of statistics.\n</summary>\n    <author>\n      <name>Yeong-Shyeong Tsai</name>\n    </author>\n    <author>\n      <name>Ying-Lin Hsu</name>\n    </author>\n    <author>\n      <name>Mung-Chung Shung</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">In this paper, new aspproaches od density estimation are studied. The\n  B-spline estimatos and the Berzier spline are stdudied carfully. The\n  consistency of the Bezier spline estimator is studied as well. There are 17\n  pages including 6 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0709.3560v7\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0709.3560v7\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0710.4242v4</id>\n    <updated>2008-05-30T08:38:00Z</updated>\n    <published>2007-10-23T11:23:46Z</published>\n    <title>Adaptive Importance Sampling in General Mixture Classes</title>\n    <summary>  In this paper, we propose an adaptive algorithm that iteratively updates both\nthe weights and component parameters of a mixture importance sampling density\nso as to optimise the importance sampling performances, as measured by an\nentropy criterion. The method is shown to be applicable to a wide class of\nimportance sampling densities, which includes in particular mixtures of\nmultivariate Student t distributions. The performances of the proposed scheme\nare studied on both artificial and real examples, highlighting in particular\nthe benefit of a novel Rao-Blackwellisation device which can be easily\nincorporated in the updating scheme.\n</summary>\n    <author>\n      <name>Olivier Capp\u00e9</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LTCI</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Randal Douc</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CMAP</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Arnaud Guillin</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LATP</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Jean-Michel Marin</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Futurs</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Christian P. Robert</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CEREMADE</arxiv:affiliation>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s11222-008-9059-x</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s11222-008-9059-x\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Removed misleading comment in Section 2</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Statistics and Computing 18, 4 (2008) 447-459</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0710.4242v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.4242v4\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0710.5098v1</id>\n    <updated>2007-10-26T14:28:08Z</updated>\n    <published>2007-10-26T14:28:08Z</published>\n    <title>Particle Filters for Multiscale Diffusions</title>\n    <summary>  We consider multiscale stochastic systems that are partially observed at\ndiscrete points of the slow time scale. We introduce a particle filter that\ntakes advantage of the multiscale structure of the system to efficiently\napproximate the optimal filter.\n</summary>\n    <author>\n      <name>Anastasia Papavasiliou</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">to appear in ESAIM Proceedings (Workshop on Sequential Monte Carlo\n  Methods: filtering and other applications, Oxford, 2006)</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0710.5098v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.5098v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0710.5670v2</id>\n    <updated>2008-03-12T23:38:20Z</updated>\n    <published>2007-10-30T15:05:58Z</published>\n    <title>An Elegant Method for Generating Multivariate Poisson Random Variable</title>\n    <summary>  Generating multivariate Poisson data is essential in many applications.\nCurrent simulation methods suffer from limitations ranging from computational\ncomplexity to restrictions on the structure of the correlation matrix. We\npropose a computationally efficient and conceptually appealing method for\ngenerating multivariate Poisson data. The method is based on simulating\nmultivariate Normal data and converting them to achieve a specific correlation\nmatrix and Poisson rate vector. This allows for generating data that have\npositive or negative correlations as well as different rates.\n</summary>\n    <author>\n      <name>Inbal Yahav</name>\n    </author>\n    <author>\n      <name>Galit Shmueli</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 11 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0710.5670v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.5670v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0711.0186v1</id>\n    <updated>2007-11-01T18:57:06Z</updated>\n    <published>2007-11-01T18:57:06Z</published>\n    <title>Population-Based Reversible Jump Markov Chain Monte Carlo</title>\n    <summary>  In this paper we present an extension of population-based Markov chain Monte\nCarlo (MCMC) to the trans-dimensional case. One of the main challenges in\nMCMC-based inference is that of simulating from high and trans-dimensional\ntarget measures. In such cases, MCMC methods may not adequately traverse the\nsupport of the target; the simulation results will be unreliable. We develop\npopulation methods to deal with such problems, and give a result proving the\nuniform ergodicity of these population algorithms, under mild assumptions. This\nresult is used to demonstrate the superiority, in terms of convergence rate, of\na population transition kernel over a reversible jump sampler for a Bayesian\nvariable selection problem. We also give an example of a population algorithm\nfor a Bayesian multivariate mixture model with an unknown number of components.\nThis is applied to gene expression data of 1000 data points in six dimensions\nand it is demonstrated that our algorithm out performs some competing Markov\nchain samplers.\n</summary>\n    <author>\n      <name>Ajay Jasra</name>\n    </author>\n    <author>\n      <name>David A. Stephens</name>\n    </author>\n    <author>\n      <name>Chris C. Holmes</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0711.0186v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0711.0186v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0801.3552v3</id>\n    <updated>2010-11-07T14:19:40Z</updated>\n    <published>2008-01-23T12:55:29Z</published>\n    <title>A statistical analysis of probabilistic counting algorithms</title>\n    <summary>  This paper considers the problem of cardinality estimation in data stream\napplications. We present a statistical analysis of probabilistic counting\nalgorithms, focusing on two techniques that use pseudo-random variates to form\nlow-dimensional data sketches. We apply conventional statistical methods to\ncompare probabilistic algorithms based on storing either selected order\nstatistics, or random projections. We derive estimators of the cardinality in\nboth cases, and show that the maximal-term estimator is recursively computable\nand has exponentially decreasing error bounds. Furthermore, we show that the\nestimators have comparable asymptotic efficiency, and explain this result by\ndemonstrating an unexpected connection between the two approaches.\n</summary>\n    <author>\n      <name>Peter Clifford</name>\n    </author>\n    <author>\n      <name>Ioana A. Cosma</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">19 pages, 0 figures</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Scandinavian Journal of Statistics, 39, 1, 1-14, 2012</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0801.3552v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0801.3552v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0801.3559v1</id>\n    <updated>2008-01-23T12:00:10Z</updated>\n    <published>2008-01-23T12:00:10Z</published>\n    <title>Efficient l_{alpha} Distance Approximation for High Dimensional Data\n  Using alpha-Stable Projection</title>\n    <summary>  In recent years, large high-dimensional data sets have become commonplace in\na wide range of applications in science and commerce. Techniques for dimension\nreduction are of primary concern in statistical analysis. Projection methods\nplay an important role. We investigate the use of projection algorithms that\nexploit properties of the alpha-stable distributions. We show that l_{alpha}\ndistances and quasi-distances can be recovered from random projections with\nfull statistical efficiency by L-estimation. The computational requirements of\nour algorithm are modest; after a once-and-for-all calculation to determine an\narray of length k, the algorithm runs in O(k) time for each distance, where k\nis the reduced dimension of the projection.\n</summary>\n    <author>\n      <name>Peter Clifford</name>\n    </author>\n    <author>\n      <name>Ioana A. Cosma</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures, submitted to COMPSTAT2008</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0801.3559v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0801.3559v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0802.3690v1</id>\n    <updated>2008-02-25T20:13:58Z</updated>\n    <published>2008-02-25T20:13:58Z</published>\n    <title>On variance stabilisation by double Rao-Blackwellisation</title>\n    <summary>  Population Monte Carlo has been introduced as a sequential importance\nsampling technique to overcome poor fit of the importance function. In this\npaper, we compare the performances of the original Population Monte Carlo\nalgorithm with a modified version that eliminates the influence of the\ntransition particle via a double Rao-Blackwellisation. This modification is\nshown to improve the exploration of the modes through an large simulation\nexperiment on posterior distributions of mean mixtures of distributions.\n</summary>\n    <author>\n      <name>Alessandra Iacobucci</name>\n    </author>\n    <author>\n      <name>Jean-Michel Marin</name>\n    </author>\n    <author>\n      <name>Christian Robert</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0802.3690v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0802.3690v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0803.2394v1</id>\n    <updated>2008-03-17T06:44:24Z</updated>\n    <published>2008-03-17T06:44:24Z</published>\n    <title>The adjusted Viterbi training for hidden Markov models</title>\n    <summary>  The EM procedure is a principal tool for parameter estimation in the hidden\nMarkov models. However, applications replace EM by Viterbi extraction, or\ntraining (VT). VT is computationally less intensive, more stable and has more\nof an intuitive appeal, but VT estimation is biased and does not satisfy the\nfollowing fixed point property. Hypothetically, given an infinitely large\nsample and initialized to the true parameters, VT will generally move away from\nthe initial values. We propose adjusted Viterbi training (VA), a new method to\nrestore the fixed point property and thus alleviate the overall imprecision of\nthe VT estimators, while preserving the computational advantages of the\nbaseline VT algorithm. Simulations elsewhere have shown that VA appreciably\nimproves the precision of estimation in both the special case of mixture models\nand more general HMMs. However, being entirely analytic, the VA correction\nrelies on infinite Viterbi alignments and associated limiting probability\ndistributions. While explicit in the mixture case, the existence of these\nlimiting measures is not obvious for more general HMMs. This paper proves that\nunder certain mild conditions, the required limiting distributions for general\nHMMs do exist.\n</summary>\n    <author>\n      <name>J\u00fcri Lember</name>\n    </author>\n    <author>\n      <name>Alexey Koloydenko</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.3150/07-BEJ105</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.3150/07-BEJ105\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in at http://dx.doi.org/10.3150/07-BEJ105 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Bernoulli 2008, Vol. 14, No. 1, 180-206</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0803.2394v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0803.2394v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0804.0390v1</id>\n    <updated>2008-04-02T16:24:48Z</updated>\n    <published>2008-04-02T16:24:48Z</published>\n    <title>A practical procedure to find matching priors for frequentist inference</title>\n    <summary>  In the manuscript, we present a practical way to find the matching priors\nproposed by Welch &amp; Peers (1963) and Peers (1965). We investigate the use of\nsaddlepoint approximations combined with matching priors and obtain p-values of\nthe test of an interest parameter in the presence of nuisance parameter. The\nadvantage of our procedure is the flexibility of choosing different initial\nconditions so that one can adjust the performance of the test. Two examples\nhave been studied, with coverage verified via Monte Carlo simulation. One\nrelates to the ratio of two exponential means, and the other relates the\nlogistic regression model. Particularly, we are interested in small sample\nsettings.\n</summary>\n    <author>\n      <name>Juan Zhang</name>\n    </author>\n    <author>\n      <name>John E. Kolassa</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, 2 tables</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0804.0390v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0804.0390v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0805.1971v2</id>\n    <updated>2008-12-12T09:15:21Z</updated>\n    <published>2008-05-14T06:30:55Z</published>\n    <title>Confidence regions for the multinomial parameter with small sample size</title>\n    <summary>  Consider the observation of n iid realizations of an experiment with d&gt;1\npossible outcomes, which corresponds to a single observation of a multinomial\ndistribution M(n,p) where p is an unknown discrete distribution on {1,...,d}.\nIn many applications, the construction of a confidence region for p when n is\nsmall is crucial. This concrete challenging problem has a long history. It is\nwell known that the confidence regions built from asymptotic statistics do not\nhave good coverage when n is small. On the other hand, most available methods\nproviding non-asymptotic regions with controlled coverage are limited to the\nbinomial case d=2. In the present work, we propose a new method valid for any\nd&gt;1. This method provides confidence regions with controlled coverage and small\nvolume, and consists of the inversion of the \"covering collection\"' associated\nwith level-sets of the likelihood. The behavior when d/n tends to infinity\nremains an interesting open problem beyond the scope of this work.\n</summary>\n    <author>\n      <name>Djalil Chafai</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">UPTE, IMT</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Didier Concordet</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">UPTE, IMT</arxiv:affiliation>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1198/jasa.2009.tm08152</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1198/jasa.2009.tm08152\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for publication in Journal of the American Statistical\n  Association (JASA)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of the American Statistical Association 104, 1071-1079\n  (2009)</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0805.1971v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0805.1971v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0805.2256v9</id>\n    <updated>2009-03-28T14:03:51Z</updated>\n    <published>2008-05-15T12:34:17Z</published>\n    <title>Adaptive approximate Bayesian computation</title>\n    <summary>  Sequential techniques can enhance the efficiency of the approximate Bayesian\ncomputation algorithm, as in Sisson et al.'s (2007) partial rejection control\nversion. While this method is based upon the theoretical works of Del Moral et\nal. (2006), the application to approximate Bayesian computation results in a\nbias in the approximation to the posterior. An alternative version based on\ngenuine importance sampling arguments bypasses this difficulty, in connection\nwith the population Monte Carlo method of Cappe et al. (2004), and it includes\nan automatic scaling of the forward kernel. When applied to a population\ngenetics example, it compares favourably with two other versions of the\napproximate algorithm.\n</summary>\n    <author>\n      <name>Mark A. Beaumont</name>\n    </author>\n    <author>\n      <name>Jean-Marie Cornuet</name>\n    </author>\n    <author>\n      <name>Jean-Michel Marin</name>\n    </author>\n    <author>\n      <name>Christian P. Robert</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1093/biomet/asp052</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1093/biomet/asp052\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 2 figures, one algorithm, third revised resubmission to\n  Biometrika</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Biometrika 96(4), 983-990, 2009</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0805.2256v9\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0805.2256v9\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0805.3079v1</id>\n    <updated>2008-05-20T13:38:03Z</updated>\n    <published>2008-05-20T13:38:03Z</published>\n    <title>A note on the ABC-PRC algorithm of Sissons et al. (2007)</title>\n    <summary>  This note describes the results of some tests of the ABC-PRC algorithm of\nSissons et al. (PNAS, 2007), and demonstrates with a toy example that the\nmethod does not converge on the true posterior distribution.\n</summary>\n    <author>\n      <name>Mark A. Beaumont</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This note was circulated to colleagues 11 October 2007. The PDF\n  document is 14 pages. There are 11 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0805.3079v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0805.3079v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0805.3602v2</id>\n    <updated>2009-02-13T14:47:14Z</updated>\n    <published>2008-05-23T09:34:11Z</published>\n    <title>Marginal Likelihood Integrals for Mixtures of Independence Models</title>\n    <summary>  Inference in Bayesian statistics involves the evaluation of marginal\nlikelihood integrals. We present algebraic algorithms for computing such\nintegrals exactly for discrete data of small sample size. Our methods apply to\nboth uniform priors and Dirichlet priors. The underlying statistical models are\nmixtures of independent distributions, or, in geometric language, secant\nvarieties of Segre-Veronese varieties.\n</summary>\n    <author>\n      <name>Shaowei Lin</name>\n    </author>\n    <author>\n      <name>Bernd Sturmfels</name>\n    </author>\n    <author>\n      <name>Zhiqiang Xu</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">28 pages. Journal of Machine Learning Research, to appear</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0805.3602v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0805.3602v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0807.0725v2</id>\n    <updated>2008-09-17T06:35:28Z</updated>\n    <published>2008-07-04T11:36:38Z</published>\n    <title>Case-deletion importance sampling estimators: Central limit theorems and\n  related results</title>\n    <summary>  Case-deleted analysis is a popular method for evaluating the influence of a\nsubset of cases on inference. The use of Monte Carlo estimation strategies in\ncomplicated Bayesian settings leads naturally to the use of importance sampling\ntechniques to assess the divergence between full-data and case-deleted\nposteriors and to provide estimates under the case-deleted posteriors. However,\nthe dependability of the importance sampling estimators depends critically on\nthe variability of the case-deleted weights. We provide theoretical results\nconcerning the assessment of the dependability of case-deleted importance\nsampling estimators in several Bayesian models. In particular, these results\nallow us to establish whether or not the estimators satisfy a central limit\ntheorem. Because the conditions we derive are of a simple analytical nature,\nthe assessment of the dependability of the estimators can be verified routinely\nbefore estimation is performed. We illustrate the use of the results in several\nexamples.\n</summary>\n    <author>\n      <name>Ilenia Epifani</name>\n    </author>\n    <author>\n      <name>Steven N. MacEachern</name>\n    </author>\n    <author>\n      <name>Mario Peruggia</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/08-EJS259</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/08-EJS259\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in at http://dx.doi.org/10.1214/08-EJS259 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Electronic Journal of Statistics 2008, Vol. 2, 774-806</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0807.0725v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0807.0725v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62F15, 62J20 (Primary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0809.2274v4</id>\n    <updated>2009-07-05T21:27:06Z</updated>\n    <published>2008-09-12T19:38:02Z</published>\n    <title>A randomized algorithm for principal component analysis</title>\n    <summary>  Principal component analysis (PCA) requires the computation of a low-rank\napproximation to a matrix containing the data being analyzed. In many\napplications of PCA, the best possible accuracy of any rank-deficient\napproximation is at most a few digits (measured in the spectral norm, relative\nto the spectral norm of the matrix being approximated). In such circumstances,\nefficient algorithms have not come with guarantees of good accuracy, unless one\nor both dimensions of the matrix being approximated are small. We describe an\nefficient algorithm for the low-rank approximation of matrices that produces\naccuracy very close to the best possible, for matrices of arbitrary sizes. We\nillustrate our theoretical results via several numerical examples.\n</summary>\n    <author>\n      <name>Vladimir Rokhlin</name>\n    </author>\n    <author>\n      <name>Arthur Szlam</name>\n    </author>\n    <author>\n      <name>Mark Tygert</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages, 6 tables, 1 figure; to appear in the SIAM Journal on Matrix\n  Analysis and Applications</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A randomized algorithm for principal component analysis, SIAM\n  Journal on Matrix Analysis and Applications, 31 (3): 1100-1124, 2009</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0809.2274v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0809.2274v4\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0809.4047v1</id>\n    <updated>2008-09-23T23:35:02Z</updated>\n    <published>2008-09-23T23:35:02Z</published>\n    <title>Improved Sequential Stopping Rule for Monte Carlo Simulation</title>\n    <summary>  This paper presents an improved result on the negative-binomial Monte Carlo\ntechnique analyzed in a previous paper for the estimation of an unknown\nprobability p. Specifically, the confidence level associated to a relative\ninterval [p/\\mu_2, p\\mu_1], with \\mu_1, \\mu_2 &gt; 1, is proved to exceed its\nasymptotic value for a broader range of intervals than that given in the\nreferred paper, and for any value of p. This extends the applicability of the\nestimator, relaxing the conditions that guarantee a given confidence level.\n</summary>\n    <author>\n      <name>Luis Mendo</name>\n    </author>\n    <author>\n      <name>Jose M. Hernando</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 figures. Paper accepted in IEEE Transactions on Communications</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0809.4047v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0809.4047v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0809.4654v1</id>\n    <updated>2008-09-26T15:12:29Z</updated>\n    <published>2008-09-26T15:12:29Z</published>\n    <title>Smooth supersaturated models</title>\n    <summary>  In areas such as kernel smoothing and non-parametric regression there is\nemphasis on smooth interpolation and smooth statistical models. Splines are\nknown to have optimal smoothness properties in one and higher dimensions. It is\nshown, with special attention to polynomial models, that smooth interpolators\ncan be constructed by first extending the monomial basis and then minimising a\nmeasure of smoothness with respect to the free parameters in the extended\nbasis. Algebraic methods are a help in choosing the extended basis which can\nalso be found as a saturated basis for an extended experimental design with\ndummy design points. One can get arbitrarily close to optimal smoothing for any\ndimension and over any region, giving a simple alternative models of spline\ntype. The relationship to splines is shown in one and two dimensions. A case\nstudy is given which includes benchmarking against kriging methods.\n</summary>\n    <author>\n      <name>Ron A. Bates</name>\n    </author>\n    <author>\n      <name>Hugo Maruri-Aguilar</name>\n    </author>\n    <author>\n      <name>Henry P. Wynn</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0809.4654v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0809.4654v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0810.1163v1</id>\n    <updated>2008-10-07T11:28:00Z</updated>\n    <published>2008-10-07T11:28:00Z</published>\n    <title>Generalised linear mixed model analysis via sequential Monte Carlo\n  sampling</title>\n    <summary>  We present a sequential Monte Carlo sampler algorithm for the Bayesian\nanalysis of generalised linear mixed models (GLMMs). These models support a\nvariety of interesting regression-type analyses, but performing inference is\noften extremely difficult, even when using the Bayesian approach combined with\nMarkov chain Monte Carlo (MCMC). The Sequential Monte Carlo sampler (SMC) is a\nnew and general method for producing samples from posterior distributions. In\nthis article we demonstrate use of the SMC method for performing inference for\nGLMMs. We demonstrate the effectiveness of the method on both simulated and\nreal data, and find that sequential Monte Carlo is a competitive alternative to\nthe available MCMC techniques.\n</summary>\n    <author>\n      <name>Y. Fan</name>\n    </author>\n    <author>\n      <name>D. S. Leslie</name>\n    </author>\n    <author>\n      <name>M. P. Wand</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/07-EJS158</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/07-EJS158\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in at http://dx.doi.org/10.1214/07-EJS158 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Electronic Journal of Statistics 2008, Vol. 2, 916-938</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0810.1163v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0810.1163v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0811.2843v1</id>\n    <updated>2008-11-18T08:57:00Z</updated>\n    <published>2008-11-18T08:57:00Z</published>\n    <title>An Algorithm for Unconstrained Quadratically Penalized Convex\n  Optimization</title>\n    <summary>  A descent algorithm, \"Quasi-Quadratic Minimization with Memory\" (QQMM), is\nproposed for unconstrained minimization of the sum, $F$, of a non-negative\nconvex function, $V$, and a quadratic form. Such problems come up in\nregularized estimation in machine learning and statistics. In addition to\nvalues of $F$, QQMM requires the (sub)gradient of $V$. Two features of QQMM\nhelp keep low the number of evaluations of the objective function it needs.\nFirst, QQMM provides good control over stopping the iterative search. This\nfeature makes QQMM well adapted to statistical problems because in such\nproblems the objective function is based on random data and therefore stopping\nearly is sensible. Secondly, QQMM uses a complex method for determining trial\nminimizers of $F$. After a description of the problem and algorithm a\nsimulation study comparing QQMM to the popular BFGS optimization algorithm is\ndescribed. The simulation study and other experiments suggest that QQMM is\ngenerally substantially faster than BFGS in the problem domain for which it was\ndesigned. A QQMM-BFGS hybrid is also generally substantially faster than BFGS\nbut does better than QQMM when QQMM is very slow.\n</summary>\n    <author>\n      <name>Steven P. Ellis</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0811.2843v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.2843v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"65K10 (Primary) 62-04 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0811.4095v3</id>\n    <updated>2009-09-02T09:02:37Z</updated>\n    <published>2008-11-25T14:27:06Z</published>\n    <title>Grapham: Graphical Models with Adaptive Random Walk Metropolis\n  Algorithms</title>\n    <summary>  Recently developed adaptive Markov chain Monte Carlo (MCMC) methods have been\napplied successfully to many problems in Bayesian statistics. Grapham is a new\nopen source implementation covering several such methods, with emphasis on\ngraphical models for directed acyclic graphs. The implemented algorithms\ninclude the seminal Adaptive Metropolis algorithm adjusting the proposal\ncovariance according to the history of the chain and a Metropolis algorithm\nadjusting the proposal scale based on the observed acceptance probability.\nDifferent variants of the algorithms allow one, for example, to use these two\nalgorithms together, employ delayed rejection and adjust several parameters of\nthe algorithms. The implemented Metropolis-within-Gibbs update allows arbitrary\nsampling blocks. The software is written in C and uses a simple extension\nlanguage Lua in configuration.\n</summary>\n    <author>\n      <name>Matti Vihola</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.csda.2009.09.001</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.csda.2009.09.001\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 3 figures; added references, revised language, other minor\n  changes</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Computational Statistics &amp; Data Analysis 54 (2010) 49-54</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0811.4095v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.4095v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0901.0876v1</id>\n    <updated>2009-01-07T17:07:52Z</updated>\n    <published>2009-01-07T17:07:52Z</published>\n    <title>A Fast Algorithm for Robust Regression with Penalised Trimmed Squares</title>\n    <summary>  The presence of groups containing high leverage outliers makes linear\nregression a difficult problem due to the masking effect. The available high\nbreakdown estimators based on Least Trimmed Squares often do not succeed in\ndetecting masked high leverage outliers in finite samples.\n  An alternative to the LTS estimator, called Penalised Trimmed Squares (PTS)\nestimator, was introduced by the authors in \\cite{ZiouAv:05,ZiAvPi:07} and it\nappears to be less sensitive to the masking problem. This estimator is defined\nby a Quadratic Mixed Integer Programming (QMIP) problem, where in the objective\nfunction a penalty cost for each observation is included which serves as an\nupper bound on the residual error for any feasible regression line. Since the\nPTS does not require presetting the number of outliers to delete from the data\nset, it has better efficiency with respect to other estimators. However, due to\nthe high computational complexity of the resulting QMIP problem, exact\nsolutions for moderately large regression problems is infeasible.\n  In this paper we further establish the theoretical properties of the PTS\nestimator, such as high breakdown and efficiency, and propose an approximate\nalgorithm called Fast-PTS to compute the PTS estimator for large data sets\nefficiently. Extensive computational experiments on sets of benchmark instances\nwith varying degrees of outlier contamination, indicate that the proposed\nalgorithm performs well in identifying groups of high leverage outliers in\nreasonable computational time.\n</summary>\n    <author>\n      <name>L. Pitsoulis</name>\n    </author>\n    <author>\n      <name>G. Zioutas</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s00180-010-0196-2</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s00180-010-0196-2\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">27 pages</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Computational Statistics, Volume 25, Number 4, 663-689, 2010</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0901.0876v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0901.0876v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0903.5292v2</id>\n    <updated>2009-03-30T20:40:14Z</updated>\n    <published>2009-03-30T19:47:24Z</published>\n    <title>A Mixture-Based Approach to Regional Adaptation for MCMC</title>\n    <summary>  Recent advances in adaptive Markov chain Monte Carlo (AMCMC) include the need\nfor regional adaptation in situations when the optimal transition kernel is\ndifferent across different regions of the sample space. Motivated by these\nfindings, we propose a mixture-based approach to determine the partition needed\nfor regional AMCMC. The mixture model is fitted using an online EM algorithm\n(see Andrieu and Moulines, 2006) which allows us to bypass simultaneously the\nheavy computational load and to implement the regional adaptive algorithm with\nonline recursion (RAPTOR). The method is tried on simulated as well as real\ndata examples.\n</summary>\n    <author>\n      <name>Radu V. Craiu</name>\n    </author>\n    <author>\n      <name>Antonio Fabio Di Narzo</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1198/jcgs.2010.09035</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1198/jcgs.2010.09035\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Computational and Graphical Statistics, 2011</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0903.5292v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0903.5292v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0904.0779v1</id>\n    <updated>2009-04-05T14:22:51Z</updated>\n    <published>2009-04-05T14:22:51Z</published>\n    <title>Least-Squares Joint Diagonalization of a matrix set by a congruence\n  transformation</title>\n    <summary>  The approximate joint diagonalization (AJD) is an important analytic tool at\nthe base of numerous independent component analysis (ICA) and other blind\nsource separation (BSS) methods, thus finding more and more applications in\nmedical imaging analysis. In this work we present a new AJD algorithm named\nSDIAG (Spheric Diagonalization). It imposes no constraint either on the input\nmatrices or on the joint diagonalizer to be estimated, thus it is very general.\nWhereas it is well grounded on the classical leastsquares criterion, a new\nnormalization reveals a very simple form of the solution matrix. Numerical\nsimulations shown that the algorithm, named SDIAG (spheric diagonalization),\nbehaves well as compared to state-of-the art AJD algorithms.\n</summary>\n    <author>\n      <name>Marco Congedo</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">GIPSA-lab</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Dinh-Tuan Pham</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LJK</arxiv:affiliation>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2nd Singaporean-French IPAL Symposium, Singapour : Singapour (2009)</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0904.0779v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.0779v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0904.1300v1</id>\n    <updated>2009-04-08T10:07:23Z</updated>\n    <published>2009-04-08T10:07:23Z</published>\n    <title>Generalized Rejection Sampling Schemes and Applications in Signal\n  Processing</title>\n    <summary>  Bayesian methods and their implementations by means of sophisticated Monte\nCarlo techniques, such as Markov chain Monte Carlo (MCMC) and particle filters,\nhave become very popular in signal processing over the last years. However, in\nmany problems of practical interest these techniques demand procedures for\nsampling from probability distributions with non-standard forms, hence we are\noften brought back to the consideration of fundamental simulation algorithms,\nsuch as rejection sampling (RS). Unfortunately, the use of RS techniques\ndemands the calculation of tight upper bounds for the ratio of the target\nprobability density function (pdf) over the proposal density from which\ncandidate samples are drawn. Except for the class of log-concave target pdf's,\nfor which an efficient algorithm exists, there are no general methods to\nanalytically determine this bound, which has to be derived from scratch for\neach specific case. In this paper, we introduce new schemes for (a) obtaining\nupper bounds for likelihood functions and (b) adaptively computing proposal\ndensities that approximate the target pdf closely. The former class of methods\nprovides the tools to easily sample from a posteriori probability distributions\n(that appear very often in signal processing problems) by drawing candidates\nfrom the prior distribution. However, they are even more useful when they are\nexploited to derive the generalized adaptive RS (GARS) algorithm introduced in\nthe second part of the paper. The proposed GARS method yields a sequence of\nproposal densities that converge towards the target pdf and enable a very\nefficient sampling of a broad class of probability distributions, possibly with\nmultiple modes and non-standard forms.\n</summary>\n    <author>\n      <name>Luca Martino</name>\n    </author>\n    <author>\n      <name>Joaquin Miguez</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.sigpro.2010.04.025</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.sigpro.2010.04.025\" rel=\"related\"/>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Signal Processing, Volume 90, Issue 11, Pages 2981-2995, 2010</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0904.1300v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.1300v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0904.2435v1</id>\n    <updated>2009-04-16T06:06:27Z</updated>\n    <published>2009-04-16T06:06:27Z</published>\n    <title>Computation of confidence intervals in regression utilizing uncertain\n  prior information</title>\n    <summary>  We consider a linear regression model with regression parameter beta\n=(beta_1, ..., beta_p) and independent and identically N(0, sigma^2)distributed\nerrors. Suppose that the parameter of interest is theta = a^T beta where a is a\nspecified vector. Define the parameter tau = c^T beta - t where the vector c\nand the number t are specified and a and c are linearly independent. Also\nsuppose that we have uncertain prior information that tau = 0. Kabaila and Giri\n(2009c) present a new frequentist 1-alpha confidence interval for theta that\nutilizes this prior information. This interval has expected length that (a) is\nrelatively small when the prior information about tau is correct and (b) has a\nmaximum value that is not too large. It coincides with the standard 1-alpha\nconfidence interval (obtained by fitting the full model to the data) when the\ndata strongly contradicts the prior information. At first sight, the\ncomputation of this new confidence interval seems to be infeasible. However, by\nthe use of the various computational devices that are presented in detail in\nthe present paper, this computation becomes feasible and practicable.\n</summary>\n    <author>\n      <name>Paul Kabaila</name>\n    </author>\n    <author>\n      <name>Khageswor Giri</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0904.2435v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.2435v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0905.2441v3</id>\n    <updated>2009-07-15T17:45:46Z</updated>\n    <published>2009-05-14T22:43:29Z</published>\n    <title>On the utility of graphics cards to perform massively parallel\n  simulation of advanced Monte Carlo methods</title>\n    <summary>  We present a case-study on the utility of graphics cards to perform massively\nparallel simulation of advanced Monte Carlo methods. Graphics cards, containing\nmultiple Graphics Processing Units (GPUs), are self-contained parallel\ncomputational devices that can be housed in conventional desktop and laptop\ncomputers. For certain classes of Monte Carlo algorithms they offer massively\nparallel simulation, with the added advantage over conventional distributed\nmulti-core processors that they are cheap, easily accessible, easy to maintain,\neasy to code, dedicated local devices with low power consumption. On a\ncanonical set of stochastic simulation examples including population-based\nMarkov chain Monte Carlo methods and Sequential Monte Carlo methods, we find\nspeedups from 35 to 500 fold over conventional single-threaded computer code.\nOur findings suggest that GPUs have the potential to facilitate the growth of\nstatistical modelling into complex data rich domains through the availability\nof cheap and accessible many-core computation. We believe the speedup we\nobserve should motivate wider use of parallelizable simulation methods and\ngreater methodological attention to their design.\n</summary>\n    <author>\n      <name>Anthony Lee</name>\n    </author>\n    <author>\n      <name>Christopher Yau</name>\n    </author>\n    <author>\n      <name>Michael B. Giles</name>\n    </author>\n    <author>\n      <name>Arnaud Doucet</name>\n    </author>\n    <author>\n      <name>Christopher C. Holmes</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1198/jcgs.2010.10039</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1198/jcgs.2010.10039\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Expansion of details; a condensed version has been submitted for\n  publication</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0905.2441v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0905.2441v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0906.1004v2</id>\n    <updated>2013-01-25T20:32:08Z</updated>\n    <published>2009-06-04T21:36:56Z</published>\n    <title>A Dynamic Programming Approach for Approximate Uniform Generation of\n  Binary Matrices with Specified Margins</title>\n    <summary>  Consider the collection of all binary matrices having a specific sequence of\nrow and column sums and consider sampling binary matrices uniformly from this\ncollection. Practical algorithms for exact uniform sampling are not known, but\nthere are practical algorithms for approximate uniform sampling. Here it is\nshown how dynamic programming and recent asymptotic enumeration results can be\nused to simplify and improve a certain class of approximate uniform samplers.\nThe dynamic programming perspective suggests interesting generalizations.\n</summary>\n    <author>\n      <name>Matthew T. Harrison</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">27 pages, minor typographic corrections from previous version,\n  superseded by arXiv:1301.3928</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0906.1004v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.1004v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0906.3215v1</id>\n    <updated>2009-06-17T15:29:16Z</updated>\n    <published>2009-06-17T15:29:16Z</published>\n    <title>Reduction algorithm for the NPMLE for the distribution function of\n  bivariate interval censored data</title>\n    <summary>  We study computational aspects of the nonparametric maximum likelihood\nestimator (NPMLE) for the distribution function of bivariate interval censored\ndata. The computation of the NPMLE consists of two steps: a parameter reduction\nstep and an optimization step. In this paper we focus on the reduction step. We\nintroduce two new reduction algorithms: the Tree algorithm and the HeightMap\nalgorithm. The Tree algorithm is only mentioned briefly. The HeightMap\nalgorithm is discussed in detail and also given in pseudo code. It is a very\nfast and simple algorithm of time complexity O(n^2). This is an order faster\nthan the best known algorithm thus far, the O(n^3) algorithm of Bogaerts and\nLesaffre (2003). We compare our algorithms with the algorithms of Gentleman and\nVandal (2001), Song (2001) and Bogaerts and Lesaffre (2003), using simulated\ndata. We show that our algorithms, and especially the HeightMap algorithm, are\nsignificantly faster. Finally, we point out that the HeightMap algorithm can be\neasily generalized to d-dimensional data with d&gt;2. Such a multivariate version\nof the HeightMap algorithm has time complexity O(n^d).\n</summary>\n    <author>\n      <name>Marloes H. Maathuis</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1198/106186005X48470</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1198/106186005X48470\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 3 figures</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Computational and Graphical Statistics 2005, Vol. 14,\n  No. 2, 352-362</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0906.3215v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.3215v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.1835v1</id>\n    <updated>2009-07-10T14:37:58Z</updated>\n    <published>2009-07-10T14:37:58Z</published>\n    <title>Information geometry for testing pseudorandom number generators</title>\n    <summary>  The information geometry of the 2-manifold of gamma probability density\nfunctions provides a framework in which pseudorandom number generators may be\nevaluated using a neighbourhood of the curve of exponential density functions.\nThe process is illustrated using the pseudorandom number generator in\nMathematica. This methodology may be useful to add to the current family of\ntest procedures in real applications to finite sampling data.\n</summary>\n    <author>\n      <name>C. T. J. Dodson</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 pages,</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.1835v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.1835v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.1997v2</id>\n    <updated>2011-10-27T11:53:22Z</updated>\n    <published>2009-07-12T13:20:32Z</published>\n    <title>Statistical estimation requires unbounded memory</title>\n    <summary>  We investigate the existence of bounded-memory consistent estimators of\nvarious statistical functionals. This question is resolved in the negative in a\nrather strong sense. We propose various bounded-memory approximations, using\ntechniques from automata theory and stochastic processes. Some questions of\npotential interest are raised for future work.\n</summary>\n    <author>\n      <name> Leonid</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Aryeh</arxiv:affiliation>\n    </author>\n    <author>\n      <name> Kontorovich</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">this is an old version, with a mistake in the proof of Thm. 6.1.\n  Please see my homepage for an updated version</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.1997v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.1997v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.3521v2</id>\n    <updated>2009-12-08T21:20:59Z</updated>\n    <published>2009-07-21T00:48:06Z</published>\n    <title>A Numerical Approach to Performance Analysis of Quickest Change-Point\n  Detection Procedures</title>\n    <summary>  For the most popular sequential change detection rules such as CUSUM, EWMA,\nand the Shiryaev-Roberts test, we develop integral equations and a concise\nnumerical method to compute a number of performance metrics, including average\ndetection delay and average time to false alarm. We pay special attention to\nthe Shiryaev-Roberts procedure and evaluate its performance for various\ninitialization strategies. Regarding the randomized initialization variant\nproposed by Pollak, known to be asymptotically optimal of order-3, we offer a\nmeans for numerically computing the quasi-stationary distribution of the\nShiryaev-Roberts statistic that is the distribution of the initializing random\nvariable, thus making this test applicable in practice. A significant\nside-product of our computational technique is the observation that\ndeterministic initializations of the Shiryaev-Roberts procedure can also enjoy\nthe same order-3 optimality property as Pollak's randomized test and, after\ncareful selection, even uniformly outperform it.\n</summary>\n    <author>\n      <name>George V. Moustakides</name>\n    </author>\n    <author>\n      <name>Aleksey S. Polunchenko</name>\n    </author>\n    <author>\n      <name>Alexander G. Tartakovsky</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">32 pages, to appear in Statistica Sinica</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Statistica Sinica, vol. 21, no. 2, pp. 571--596, 2011</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0907.3521v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.3521v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.4018v2</id>\n    <updated>2009-11-22T01:09:01Z</updated>\n    <published>2009-07-23T09:40:40Z</published>\n    <title>Simulating Events of Unknown Probabilities via Reverse Time Martingales</title>\n    <summary>  Assume that one aims to simulate an event of unknown probability $s\\in (0,1)$\nwhich is uniquely determined, however only its approximations can be obtained\nusing a finite computational effort. Such settings are often encountered in\nstatistical simulations. We consider two specific examples. First, the exact\nsimulation of non-linear diffusions, second, the celebrated Bernoulli factory\nproblem of generating an $f(p)-$coin given a sequence $X_1,X_2,...$ of\nindependent tosses of a $p-$coin (with known $f$ and unknown $p$). We describe\na general framework and provide algorithms where this kind of problems can be\nfitted and solved. The algorithms are straightforward to implement and thus\nallow for effective simulation of desired events of probability $s.$ In the\ncase of diffusions, we obtain the algorithm of \\cite{BeskosRobertsEA1} as a\nspecific instance of the generic framework developed here. In the case of the\nBernoulli factory, our work offers a statistical understanding of the\nNacu-Peres algorithm for $f(p) = \\min\\{2p, 1-2\\varepsilon\\}$ (which is central\nto the general question) and allows for its immediate implementation that\navoids algorithmic difficulties of the original version.\n</summary>\n    <author>\n      <name>Krzysztof Latuszynski</name>\n    </author>\n    <author>\n      <name>Ioannis Kosmidis</name>\n    </author>\n    <author>\n      <name>Omiros Papaspiliopoulos</name>\n    </author>\n    <author>\n      <name>Gareth O. Roberts</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">referees suggestions incorporated</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.4018v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.4018v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.5123v1</id>\n    <updated>2009-07-29T13:33:51Z</updated>\n    <published>2009-07-29T13:33:51Z</published>\n    <title>Computational methods for Bayesian model choice</title>\n    <summary>  In this note, we shortly survey some recent approaches on the approximation\nof the Bayes factor used in Bayesian hypothesis testing and in Bayesian model\nchoice. In particular, we reassess importance sampling, harmonic mean sampling,\nand nested sampling from a unified perspective.\n</summary>\n    <author>\n      <name>Christian P. Robert</name>\n    </author>\n    <author>\n      <name>Darren Wraith</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.3275622</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.3275622\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 4 figures, submitted to the proceedings of MaxEnt 2009,\n  July 05-10, 2009, to be published by the American Institute of Physics</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.5123v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.5123v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0908.4119v1</id>\n    <updated>2009-08-28T00:44:32Z</updated>\n    <published>2009-08-28T00:44:32Z</published>\n    <title>Numerical Comparison of Cusum and Shiryaev-Roberts Procedures for\n  Detecting Changes in Distributions</title>\n    <summary>  The CUSUM procedure is known to be optimal for detecting a change in\ndistribution under a minimax scenario, whereas the Shiryaev-Roberts procedure\nis optimal for detecting a change that occurs at a distant time horizon. As a\nsimpler alternative to the conventional Monte Carlo approach, we propose a\nnumerical method for the systematic comparison of the two detection schemes in\nboth settings, i.e., minimax and for detecting changes that occur in the\ndistant future. Our goal is accomplished by deriving a set of exact integral\nequations for the performance metrics, which are then solved numerically. We\npresent detailed numerical results for the problem of detecting a change in the\nmean of a Gaussian sequence, which show that the difference between the two\nprocedures is significant only when detecting small changes.\n</summary>\n    <author>\n      <name>George V. Moustakides</name>\n    </author>\n    <author>\n      <name>Aleksey S. Polunchenko</name>\n    </author>\n    <author>\n      <name>Alexander G. Tartakovsky</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1080/03610920902947774</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1080/03610920902947774\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages, 8 figures, to appear in Communications in Statistics -\n  Theory and Methods</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Communications in Statistics - Theory and Methods, vol. 38, no.\n  16-17, pp. 3225-3239, 2009</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0908.4119v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0908.4119v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0909.0389v1</id>\n    <updated>2009-09-02T11:25:53Z</updated>\n    <published>2009-09-02T11:25:53Z</published>\n    <title>Monte Carlo Methods in Statistics</title>\n    <summary>  Monte Carlo methods are now an essential part of the statistician's toolbox,\nto the point of being more familiar to graduate students than the measure\ntheoretic notions upon which they are based! We recall in this note some of the\nadvances made in the design of Monte Carlo techniques towards their use in\nStatistics, referring to Robert and Casella (2004,2010) for an in-depth\ncoverage.\n</summary>\n    <author>\n      <name>Christian P. Robert</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Entry submitted to the International Handbook of Statistical Methods</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0909.0389v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0909.0389v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0911.0412v2</id>\n    <updated>2009-11-09T22:41:21Z</updated>\n    <published>2009-11-02T21:33:22Z</published>\n    <title>Probability matrices, non-negative rank, and parameterizations of\n  mixture models</title>\n    <summary>  In this paper we parameterize non-negative matrices of sum one and rank at\nmost two. More precisely, we give a family of parameterizations using the least\npossible number of parameters. We also show how these parameterizations relate\nto a class of statistical models, known in Probability and Statistics as\nmixture models for contingency tables.\n</summary>\n    <author>\n      <name>Enrico Carlini</name>\n    </author>\n    <author>\n      <name>Fabio Rapallo</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0911.0412v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0911.0412v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0912.4729v1</id>\n    <updated>2009-12-23T20:53:12Z</updated>\n    <published>2009-12-23T20:53:12Z</published>\n    <title>Likelihood-free Bayesian inference for alpha-stable models</title>\n    <summary>  $\\alpha$-stable distributions are utilised as models for heavy-tailed noise\nin many areas of statistics, finance and signal processing engineering.\n  However, in general, neither univariate nor multivariate $\\alpha$-stable\nmodels admit closed form densities which can be evaluated pointwise. This\ncomplicates the inferential procedure.\n  As a result, $\\alpha$-stable models are practically limited to the univariate\nsetting under the Bayesian paradigm, and to bivariate models under the\nclassical framework.\n  In this article we develop a novel Bayesian approach to modelling univariate\nand multivariate $\\alpha$-stable distributions based on recent advances in\n\"likelihood-free\" inference.\n  We present an evaluation of the performance of this procedure in 1, 2 and 3\ndimensions, and provide an analysis of real daily currency exchange rate data.\nThe proposed approach provides a feasible inferential methodology at a moderate\ncomputational cost.\n</summary>\n    <author>\n      <name>G. W. Peters</name>\n    </author>\n    <author>\n      <name>S. A. Sisson</name>\n    </author>\n    <author>\n      <name>Y. Fan</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0912.4729v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0912.4729v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1001.2797v1</id>\n    <updated>2010-01-16T00:55:16Z</updated>\n    <published>2010-01-16T00:55:16Z</published>\n    <title>Adaptive Gibbs samplers</title>\n    <summary>  We consider various versions of adaptive Gibbs and Metropolis within-Gibbs\nsamplers, which update their selection probabilities (and perhaps also their\nproposal distributions) on the fly during a run, by learning as they go in an\nattempt to optimise the algorithm. We present a cautionary example of how even\na simple-seeming adaptive Gibbs sampler may fail to converge. We then present\nvarious positive results guaranteeing convergence of adaptive Gibbs samplers\nunder certain conditions.\n</summary>\n    <author>\n      <name>Krzysztof Latuszynski</name>\n    </author>\n    <author>\n      <name>Jeffrey S. Rosenthal</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1001.2797v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1001.2797v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1002.1940v1</id>\n    <updated>2010-02-09T18:44:54Z</updated>\n    <published>2010-02-09T18:44:54Z</published>\n    <title>Measures of Analysis of Time Series (MATS): A MATLAB Toolkit for\n  Computation of Multiple Measures on Time Series Data Bases</title>\n    <summary>  In many applications, such as physiology and finance, large time series data\nbases are to be analyzed requiring the computation of linear, nonlinear and\nother measures. Such measures have been developed and implemented in commercial\nand freeware softwares rather selectively and independently. The Measures of\nAnalysis of Time Series ({\\tt MATS}) {\\tt MATLAB} toolkit is designed to handle\nan arbitrary large set of scalar time series and compute a large variety of\nmeasures on them, allowing for the specification of varying measure parameters\nas well. The variety of options with added facilities for visualization of the\nresults support different settings of time series analysis, such as the\ndetection of dynamics changes in long data records, resampling (surrogate or\nbootstrap) tests for independence and linearity with various test statistics,\nand discrimination power of different measures and for different combinations\nof their parameters. The basic features of {\\tt MATS} are presented and the\nimplemented measures are briefly described. The usefulness of {\\tt MATS} is\nillustrated on some empirical examples along with screenshots.\n</summary>\n    <author>\n      <name>Dimitris Kugiumtzis</name>\n    </author>\n    <author>\n      <name>Alkiviadis Tsimpiris</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">25 pages, 9 figures, two tables, the software can be downloaded at\n  http://eeganalysis.web.auth.gr/indexen.htm</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1002.1940v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1002.1940v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1003.3201v1</id>\n    <updated>2010-03-16T17:45:16Z</updated>\n    <published>2010-03-16T17:45:16Z</published>\n    <title>Covariance-Adaptive Slice Sampling</title>\n    <summary>  We describe two slice sampling methods for taking multivariate steps using\nthe crumb framework. These methods use the gradients at rejected proposals to\nadapt to the local curvature of the log-density surface, a technique that can\nproduce much better proposals when parameters are highly correlated. We\nevaluate our methods on four distributions and compare their performance to\nthat of a non-adaptive slice sampling method and a Metropolis method. The\nadaptive methods perform favorably on low-dimensional target distributions with\nhighly-correlated parameters.\n</summary>\n    <author>\n      <name>Madeleine Thompson</name>\n    </author>\n    <author>\n      <name>Radford M. Neal</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1003.3201v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.3201v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"65C05\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1003.3272v1</id>\n    <updated>2010-03-16T23:06:35Z</updated>\n    <published>2010-03-16T23:06:35Z</published>\n    <title>Graphics Processing Units and High-Dimensional Optimization</title>\n    <summary>  This paper discusses the potential of graphics processing units (GPUs) in\nhigh-dimensional optimization problems. A single GPU card with hundreds of\narithmetic cores can be inserted in a personal computer and dramatically\naccelerates many statistical algorithms. To exploit these devices fully,\noptimization algorithms should reduce to multiple parallel tasks, each\naccessing a limited amount of data. These criteria favor EM and MM algorithms\nthat separate parameters and data. To a lesser extent block relaxation and\ncoordinate descent and ascent also qualify. We demonstrate the utility of GPUs\nin nonnegative matrix factorization, PET image reconstruction, and\nmultidimensional scaling. Speedups of 100 fold can easily be attained. Over the\nnext decade, GPUs will fundamentally alter the landscape of computational\nstatistics. It is time for more statisticians to get on-board.\n</summary>\n    <author>\n      <name>Hua Zhou</name>\n    </author>\n    <author>\n      <name>Kenneth Lange</name>\n    </author>\n    <author>\n      <name>Marc A. Suchard</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/10-STS336</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/10-STS336\" rel=\"related\"/>\n    <link href=\"http://arxiv.org/abs/1003.3272v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.3272v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1003.3357v2</id>\n    <updated>2010-03-22T12:55:08Z</updated>\n    <published>2010-03-17T12:26:05Z</published>\n    <title>Computational Methods in Bayesian Statistics</title>\n    <summary>  This paper focuses on utilizing two different Bayesian methods to deal with a\nvariety of toy problems which occur in data analysis. In particular we\nimplement the Variational Bayesian and Nested Sampling methods to tackle the\nproblems of polynomial selection and Gaussian Mixture Models, comparing the\nalgorithms in terms of processing speed and accuracy. In the problems tackled\nhere it is the Variational Bayesian algorithms which are the faster though both\nresults give similar results.\n</summary>\n    <author>\n      <name>Alan Tua</name>\n    </author>\n    <author>\n      <name>Kristian Zarb Adami</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1003.3357v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.3357v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1003.5338v3</id>\n    <updated>2017-02-13T06:01:37Z</updated>\n    <published>2010-03-28T04:49:41Z</published>\n    <title>Ideal-Theoretic Strategies for Asymptotic Approximation of Marginal\n  Likelihood Integrals</title>\n    <summary>  The accurate asymptotic evaluation of marginal likelihood integrals is a\nfundamental problem in Bayesian statistics. Following the approach introduced\nby Watanabe, we translate this into a problem of computational algebraic\ngeometry, namely, to determine the real log canonical threshold of a polynomial\nideal, and we present effective methods for solving this problem. Our results\nare based on resolution of singularities. They apply to parametric models where\nthe Kullback-Leibler distance is upper and lower bounded by scalar multiples of\nsome sum of squared real analytic functions. Such models include finite state\ndiscrete models.\n</summary>\n    <author>\n      <name>Shaowei Lin</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.18409/jas.v8i1.47</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.18409/jas.v8i1.47\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">34 pages. Moved technical parts of the computation for the statistics\n  example to the appendix</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1003.5338v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.5338v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62E99, 14E15, 62G20\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1003.5851v2</id>\n    <updated>2011-05-23T10:47:25Z</updated>\n    <published>2010-03-30T15:52:04Z</published>\n    <title>An empirical Bayes procedure for the selection of Gaussian graphical\n  models</title>\n    <summary>  A new methodology for model determination in decomposable graphical Gaussian\nmodels is developed. The Bayesian paradigm is used and, for each given graph, a\nhyper inverse Wishart prior distribution on the covariance matrix is\nconsidered. This prior distribution depends on hyper-parameters. It is\nwell-known that the models's posterior distribution is sensitive to the\nspecification of these hyper-parameters and no completely satisfactory method\nis registered. In order to avoid this problem, we suggest adopting an empirical\nBayes strategy, that is a strategy for which the values of the hyper-parameters\nare determined using the data. Typically, the hyper-parameters are fixed to\ntheir maximum likelihood estimations. In order to calculate these maximum\nlikelihood estimations, we suggest a Markov chain Monte Carlo version of the\nStochastic Approximation EM algorithm. Moreover, we introduce a new sampling\nscheme in the space of graphs that improves the add and delete proposal of\nArmstrong et al. (2009). We illustrate the efficiency of this new scheme on\nsimulated and real datasets.\n</summary>\n    <author>\n      <name>Sophie Donnet</name>\n    </author>\n    <author>\n      <name>Jean-Michel Marin</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1003.5851v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1003.5851v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1004.0784v3</id>\n    <updated>2011-05-24T16:51:52Z</updated>\n    <published>2010-04-06T07:03:34Z</published>\n    <title>Maximin design on non hypercube domain and kernel interpolation</title>\n    <summary>  In the paradigm of computer experiments, the choice of an experimental design\nis an important issue. When no information is available about the black-box\nfunction to be approximated, an exploratory design have to be used. In this\ncontext, two dispersion criteria are usually considered: the minimax and the\nmaximin ones. In the case of a hypercube domain, a standard strategy consists\nof taking the maximin design within the class of Latin hypercube designs.\nHowever, in a non hypercube context, it does not make sense to use the Latin\nhypercube strategy. Moreover, whatever the design is, the black-box function is\ntypically approximated thanks to kernel interpolation. Here, we first provide a\ntheoretical justification to the maximin criterion with respect to kernel\ninterpolations. Then, we propose simulated annealing algorithms to determine\nmaximin designs in any bounded connected domain. We prove the convergence of\nthe different schemes.\n</summary>\n    <author>\n      <name>Yves Auffray</name>\n    </author>\n    <author>\n      <name>Pierre Barbillon</name>\n    </author>\n    <author>\n      <name>Jean-Michel Marin</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">3 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/1004.0784v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1004.0784v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1004.0887v2</id>\n    <updated>2015-08-26T14:11:06Z</updated>\n    <published>2010-04-06T16:37:45Z</published>\n    <title>A pruned dynamic programming algorithm to recover the best segmentations\n  with $1$ to $K_{max}$ change-points</title>\n    <summary>  A common computational problem in multiple change-point models is to recover\nthe segmentations with $1$ to $K_{max}$ change-points of minimal cost with\nrespect to some loss function. Here we present an algorithm to prune the set of\ncandidate change-points which is based on a functional representation of the\ncost of segmentations. We study the worst case complexity of the algorithm when\nthere is a unidimensional parameter per segment and demonstrate that it is at\nworst equivalent to the complexity of the segment neighbourhood algorithm:\n$\\mathcal{O}(K_{max} n^2)$. For a particular loss function we demonstrate that\npruning is on average efficient even if there are no change-points in the\nsignal. Finally, we empirically study the performance of the algorithm in the\ncase of the quadratic loss and show that it is faster than the segment\nneighbourhood algorithm.\n</summary>\n    <author>\n      <name>Guillem Rigaill</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">31 pages, An extended version of the pre-print</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">J-Sfds Vol. 156, No 4 2015 pgs. 180-205</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/1004.0887v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1004.0887v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.CO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"