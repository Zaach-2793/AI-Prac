"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Astat.ML%26id_list%3D%26start%3D0%26max_results%3D50\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=cat:stat.ML&amp;id_list=&amp;start=0&amp;max_results=50</title>\n  <id>http://arxiv.org/api/ex7j+G8rA8xEdLxZX1ezkqFoAiw</id>\n  <updated>2025-03-22T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">69999</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">50</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/0707.3536v1</id>\n    <updated>2007-07-24T12:45:39Z</updated>\n    <published>2007-07-24T12:45:39Z</published>\n    <title>Degenerating families of dendrograms</title>\n    <summary>  Dendrograms used in data analysis are ultrametric spaces, hence objects of\nnonarchimedean geometry. It is known that there exist $p$-adic representation\nof dendrograms. Completed by a point at infinity, they can be viewed as\nsubtrees of the Bruhat-Tits tree associated to the $p$-adic projective line.\nThe implications are that certain moduli spaces known in algebraic geometry are\n$p$-adic parameter spaces of (families of) dendrograms, and stochastic\nclassification can also be handled within this framework. At the end, we\ncalculate the topology of the hidden part of a dendrogram.\n</summary>\n    <author>\n      <name>Patrick Erik Bradley</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s00357-008-9009-5</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s00357-008-9009-5\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">13 pages, 8 figures</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">J. Classif. 25, 27-42 (2008)</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0707.3536v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0707.3536v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0707.4072v1</id>\n    <updated>2007-07-27T09:37:28Z</updated>\n    <published>2007-07-27T09:37:28Z</published>\n    <title>Families of dendrograms</title>\n    <summary>  A conceptual framework for cluster analysis from the viewpoint of p-adic\ngeometry is introduced by describing the space of all dendrograms for n\ndatapoints and relating it to the moduli space of p-adic Riemannian spheres\nwith punctures using a method recently applied by Murtagh (2004b). This method\nembeds a dendrogram as a subtree into the Bruhat-Tits tree associated to the\np-adic numbers, and goes back to Cornelissen et al. (2001) in p-adic geometry.\nAfter explaining the definitions, the concept of classifiers is discussed in\nthe context of moduli spaces, and upper bounds for the number of hidden\nvertices in dendrograms are given.\n</summary>\n    <author>\n      <name>Patrick Erik Bradley</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-540-78246-9_12</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-540-78246-9_12\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 3 figures. To appear in: Proceedings of the 31st Annual\n  Conference of the German Classification Society on Data Analysis, Machine\n  Learning, and Applications, Freiburg im Breisgau, 7-9 March 2007. Springer\n  series Studies in Classification, Data Analysis, and Knowledge Organization</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0707.4072v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0707.4072v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0708.2377v1</id>\n    <updated>2007-08-17T14:41:58Z</updated>\n    <published>2007-08-17T14:41:58Z</published>\n    <title>Online Learning in Discrete Hidden Markov Models</title>\n    <summary>  We present and analyse three online algorithms for learning in discrete\nHidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm.\nUsing the Kullback-Leibler divergence as a measure of generalisation error we\ndraw learning curves in simplified situations. The performance for learning\ndrifting concepts of one of the presented algorithms is analysed and compared\nwith the Baldi-Chauvin algorithm in the same situations. A brief discussion\nabout learning and symmetry breaking based on our results is also presented.\n</summary>\n    <author>\n      <name>Roberto C. Alamino</name>\n    </author>\n    <author>\n      <name>Nestor Caticha</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.2423274</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.2423274\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 6 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0708.2377v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0708.2377v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0709.2760v3</id>\n    <updated>2007-10-16T15:30:16Z</updated>\n    <published>2007-09-18T06:45:30Z</published>\n    <title>Supervised Machine Learning with a Novel Kernel Density Estimator</title>\n    <summary>  In recent years, kernel density estimation has been exploited by computer\nscientists to model machine learning problems. The kernel density estimation\nbased approaches are of interest due to the low time complexity of either O(n)\nor O(n*log(n)) for constructing a classifier, where n is the number of sampling\ninstances. Concerning design of kernel density estimators, one essential issue\nis how fast the pointwise mean square error (MSE) and/or the integrated mean\nsquare error (IMSE) diminish as the number of sampling instances increases. In\nthis article, it is shown that with the proposed kernel function it is feasible\nto make the pointwise MSE of the density estimator converge at O(n^-2/3)\nregardless of the dimension of the vector space, provided that the probability\ndensity function at the point of interest meets certain conditions.\n</summary>\n    <author>\n      <name>Yen-Jen Oyang</name>\n    </author>\n    <author>\n      <name>Darby Tien-Hao Chang</name>\n    </author>\n    <author>\n      <name>Yu-Yen Ou</name>\n    </author>\n    <author>\n      <name>Hao-Geng Hung</name>\n    </author>\n    <author>\n      <name>Chih-Peng Wu</name>\n    </author>\n    <author>\n      <name>Chien-Yu Chen</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">The new version includes an additional theorem, Theorem 3</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0709.2760v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0709.2760v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0709.2936v1</id>\n    <updated>2007-09-18T23:56:17Z</updated>\n    <published>2007-09-18T23:56:17Z</published>\n    <title>Bayesian Classification and Regression with High Dimensional Features</title>\n    <summary>  This thesis responds to the challenges of using a large number, such as\nthousands, of features in regression and classification problems.\n  There are two situations where such high dimensional features arise. One is\nwhen high dimensional measurements are available, for example, gene expression\ndata produced by microarray techniques. For computational or other reasons,\npeople may select only a small subset of features when modelling such data, by\nlooking at how relevant the features are to predicting the response, based on\nsome measure such as correlation with the response in the training data.\nAlthough it is used very commonly, this procedure will make the response appear\nmore predictable than it actually is. In Chapter 2, we propose a Bayesian\nmethod to avoid this selection bias, with application to naive Bayes models and\nmixture models.\n  High dimensional features also arise when we consider high-order\ninteractions. The number of parameters will increase exponentially with the\norder considered. In Chapter 3, we propose a method for compressing a group of\nparameters into a single one, by exploiting the fact that many predictor\nvariables derived from high-order interactions have the same values for all the\ntraining cases. The number of compressed parameters may have converged before\nconsidering the highest possible order. We apply this compression method to\nlogistic sequence prediction models and logistic classification models.\n  We use both simulated data and real data to test our methods in both\nchapters.\n</summary>\n    <author>\n      <name>Longhai Li</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">PhD Thesis Submitted to University of Toronto, 129 Pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0709.2936v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0709.2936v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0709.2989v1</id>\n    <updated>2007-09-19T10:56:13Z</updated>\n    <published>2007-09-19T10:56:13Z</published>\n    <title>Simulated Annealing: Rigorous finite-time guarantees for optimization on\n  continuous domains</title>\n    <summary>  Simulated annealing is a popular method for approaching the solution of a\nglobal optimization problem. Existing results on its performance apply to\ndiscrete combinatorial optimization where the optimization variables can assume\nonly a finite set of possible values. We introduce a new general formulation of\nsimulated annealing which allows one to guarantee finite-time performance in\nthe optimization of functions of continuous variables. The results hold\nuniversally for any optimization problem on a bounded domain and establish a\nconnection between simulated annealing and up-to-date theory of convergence of\nMarkov chain Monte Carlo methods on continuous domains. This work is inspired\nby the concept of finite-time learning with known accuracy and confidence\ndeveloped in statistical learning theory.\n</summary>\n    <author>\n      <name>A. Lecchini-Visintini</name>\n    </author>\n    <author>\n      <name>J. Lygeros</name>\n    </author>\n    <author>\n      <name>J. Maciejowski</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 2 figures. Preprint. The final version will appear in:\n  Advances in Neural Information Processing Systems 20, Proceedings of NIPS\n  2007, MIT Press</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0709.2989v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0709.2989v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0710.0845v3</id>\n    <updated>2009-08-27T16:32:49Z</updated>\n    <published>2007-10-03T17:32:21Z</published>\n    <title>The nested Chinese restaurant process and Bayesian nonparametric\n  inference of topic hierarchies</title>\n    <summary>  We present the nested Chinese restaurant process (nCRP), a stochastic process\nwhich assigns probability distributions to infinitely-deep,\ninfinitely-branching trees. We show how this stochastic process can be used as\na prior distribution in a Bayesian nonparametric model of document collections.\nSpecifically, we present an application to information retrieval in which\ndocuments are modeled as paths down a random tree, and the preferential\nattachment dynamics of the nCRP leads to clustering of documents according to\nsharing of topics at multiple levels of abstraction. Given a corpus of\ndocuments, a posterior inference algorithm finds an approximation to a\nposterior distribution over trees, topics and allocations of words to levels of\nthe tree. We demonstrate this algorithm on collections of scientific abstracts\nfrom several journals. This model exemplifies a recent trend in statistical\nmachine learning--the use of Bayesian nonparametric methods to infer\ndistributions on flexible data structures.\n</summary>\n    <author>\n      <name>David M. Blei</name>\n    </author>\n    <author>\n      <name>Thomas L. Griffiths</name>\n    </author>\n    <author>\n      <name>Michael I. Jordan</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0710.0845v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.0845v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0710.3183v1</id>\n    <updated>2007-10-16T21:16:29Z</updated>\n    <published>2007-10-16T21:16:29Z</published>\n    <title>Probabilistic coherence and proper scoring rules</title>\n    <summary>  We provide self-contained proof of a theorem relating probabilistic coherence\nof forecasts to their non-domination by rival forecasts with respect to any\nproper scoring rule. The theorem appears to be new but is closely related to\nresults achieved by other investigators.\n</summary>\n    <author>\n      <name>Joel Predd</name>\n    </author>\n    <author>\n      <name>Robert Seiringer</name>\n    </author>\n    <author>\n      <name>Elliott H. Lieb</name>\n    </author>\n    <author>\n      <name>Daniel Osherson</name>\n    </author>\n    <author>\n      <name>Vincent Poor</name>\n    </author>\n    <author>\n      <name>Sanjeev Kulkarni</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TIT.2009.2027573</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TIT.2009.2027573\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LaTeX2, 15 pages</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE T. Inform. Theory 55, 4786 (2009)</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0710.3183v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.3183v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0710.3742v1</id>\n    <updated>2007-10-19T17:18:30Z</updated>\n    <published>2007-10-19T17:18:30Z</published>\n    <title>Bayesian Online Changepoint Detection</title>\n    <summary>  Changepoints are abrupt variations in the generative parameters of a data\nsequence. Online detection of changepoints is useful in modelling and\nprediction of time series in application areas such as finance, biometrics, and\nrobotics. While frequentist methods have yielded online filtering and\nprediction techniques, most Bayesian papers have focused on the retrospective\nsegmentation problem. Here we examine the case where the model parameters\nbefore and after the changepoint are independent and we derive an online\nalgorithm for exact inference of the most recent changepoint. We compute the\nprobability distribution of the length of the current ``run,'' or time since\nthe last changepoint, using a simple message-passing algorithm. Our\nimplementation is highly modular so that the algorithm may be applied to a\nvariety of types of data. We illustrate this modularity by demonstrating the\nalgorithm on three different real-world data sets.\n</summary>\n    <author>\n      <name>Ryan Prescott Adams</name>\n    </author>\n    <author>\n      <name>David J. C. MacKay</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 4 figures, latex</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0710.3742v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0710.3742v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0711.2434v1</id>\n    <updated>2007-11-15T15:09:41Z</updated>\n    <published>2007-11-15T15:09:41Z</published>\n    <title>Variable importance in binary regression trees and forests</title>\n    <summary>  We characterize and study variable importance (VIMP) and pairwise variable\nassociations in binary regression trees. A key component involves the node mean\nsquared error for a quantity we refer to as a maximal subtree. The theory\nnaturally extends from single trees to ensembles of trees and applies to\nmethods like random forests. This is useful because while importance values\nfrom random forests are used to screen variables, for example they are used to\nfilter high throughput genomic data in Bioinformatics, very little theory\nexists about their properties.\n</summary>\n    <author>\n      <name>Hemant Ishwaran</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/07-EJS039</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/07-EJS039\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in at http://dx.doi.org/10.1214/07-EJS039 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Electronic Journal of Statistics 2007, Vol. 1, 519-537</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0711.2434v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0711.2434v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0712.0248v1</id>\n    <updated>2007-12-03T13:49:36Z</updated>\n    <published>2007-12-03T13:49:36Z</published>\n    <title>Pac-Bayesian Supervised Classification: The Thermodynamics of\n  Statistical Learning</title>\n    <summary>  This monograph deals with adaptive supervised classification, using tools\nborrowed from statistical mechanics and information theory, stemming from the\nPACBayesian approach pioneered by David McAllester and applied to a conception\nof statistical learning theory forged by Vladimir Vapnik. Using convex analysis\non the set of posterior probability measures, we show how to get local measures\nof the complexity of the classification model involving the relative entropy of\nposterior distributions with respect to Gibbs posterior measures. We then\ndiscuss relative bounds, comparing the generalization error of two\nclassification rules, showing how the margin assumption of Mammen and Tsybakov\ncan be replaced with some empirical measure of the covariance structure of the\nclassification model.We show how to associate to any posterior distribution an\neffective temperature relating it to the Gibbs prior distribution with the same\nlevel of expected error rate, and how to estimate this effective temperature\nfrom data, resulting in an estimator whose expected error rate converges\naccording to the best possible power of the sample size adaptively under any\nmargin and parametric complexity assumptions. We describe and study an\nalternative selection scheme based on relative bounds between estimators, and\npresent a two step localization technique which can handle the selection of a\nparametric model from a family of those. We show how to extend systematically\nall the results obtained in the inductive setting to transductive learning, and\nuse this to improve Vapnik's generalization bounds, extending them to the case\nwhen the sample is made of independent non-identically distributed pairs of\npatterns and labels. Finally we review briefly the construction of Support\nVector Machines and show how to derive generalization bounds for them,\nmeasuring the complexity either through the number of support vectors or\nthrough the value of the transductive or inductive margin.\n</summary>\n    <author>\n      <name>Olivier Catoni</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/074921707000000391</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/074921707000000391\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in at http://dx.doi.org/10.1214/074921707000000391 the IMS\n  Lecture Notes Monograph Series\n  (http://www.imstat.org/publications/lecnotes.htm) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IMS Lecture Notes Monograph Series 2007, Vol. 56, i-xii, 1-163</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0712.0248v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0712.0248v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62H30, 68T05, 62B10 (Primary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0802.2906v2</id>\n    <updated>2008-02-21T17:52:16Z</updated>\n    <published>2008-02-20T18:26:31Z</published>\n    <title>Classification Constrained Dimensionality Reduction</title>\n    <summary>  Dimensionality reduction is a topic of recent interest. In this paper, we\npresent the classification constrained dimensionality reduction (CCDR)\nalgorithm to account for label information. The algorithm can account for\nmultiple classes as well as the semi-supervised setting. We present an\nout-of-sample expressions for both labeled and unlabeled data. For unlabeled\ndata, we introduce a method of embedding a new point as preprocessing to a\nclassifier. For labeled data, we introduce a method that improves the embedding\nduring the training phase using the out-of-sample extension. We investigate\nclassification performance using the CCDR algorithm on hyper-spectral satellite\nimagery data. We demonstrate the performance gain for both local and global\nclassifiers and demonstrate a 10% improvement of the $k$-nearest neighbors\nalgorithm performance. We present a connection between intrinsic dimension\nestimation and the optimal embedding dimension obtained using the CCDR\nalgorithm.\n</summary>\n    <author>\n      <name>Raviv Raich</name>\n    </author>\n    <author>\n      <name>Jose A. Costa</name>\n    </author>\n    <author>\n      <name>Steven B. Damelin</name>\n    </author>\n    <author>\n      <name>Alfred O. Hero III</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0802.2906v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0802.2906v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0803.1628v1</id>\n    <updated>2008-03-11T18:38:52Z</updated>\n    <published>2008-03-11T18:38:52Z</published>\n    <title>Component models for large networks</title>\n    <summary>  Being among the easiest ways to find meaningful structure from discrete data,\nLatent Dirichlet Allocation (LDA) and related component models have been\napplied widely. They are simple, computationally fast and scalable,\ninterpretable, and admit nonparametric priors. In the currently popular field\nof network modeling, relatively little work has taken uncertainty of data\nseriously in the Bayesian sense, and component models have been introduced to\nthe field only recently, by treating each node as a bag of out-going links. We\nintroduce an alternative, interaction component model for communities (ICMc),\nwhere the whole network is a bag of links, stemming from different components.\nThe former finds both disassortative and assortative structure, while the\nalternative assumes assortativity and finds community-like structures like the\nearlier methods motivated by physics. With Dirichlet Process priors and an\nefficient implementation the models are highly scalable, as demonstrated with a\nsocial network from the Last.fm web site, with 670,000 nodes and 1.89 million\nlinks.\n</summary>\n    <author>\n      <name>Janne Sinkkonen</name>\n    </author>\n    <author>\n      <name>Janne Aukia</name>\n    </author>\n    <author>\n      <name>Samuel Kaski</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0803.1628v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0803.1628v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0804.1026v1</id>\n    <updated>2008-04-07T13:46:27Z</updated>\n    <published>2008-04-07T13:46:27Z</published>\n    <title>Testing for Homogeneity with Kernel Fisher Discriminant Analysis</title>\n    <summary>  We propose to investigate test statistics for testing homogeneity in\nreproducing kernel Hilbert spaces. Asymptotic null distributions under null\nhypothesis are derived, and consistency against fixed and local alternatives is\nassessed. Finally, experimental evidence of the performance of the proposed\napproach on both artificial data and a speaker verification task is provided.\n</summary>\n    <author>\n      <name>Zaid Harchaoui</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LTCI</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Francis Bach</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Rocquencourt</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Eric Moulines</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LTCI</arxiv:affiliation>\n    </author>\n    <link href=\"http://arxiv.org/abs/0804.1026v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0804.1026v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0804.1325v1</id>\n    <updated>2008-04-08T16:58:11Z</updated>\n    <published>2008-04-08T16:58:11Z</published>\n    <title>On the underestimation of model uncertainty by Bayesian K-nearest\n  neighbors</title>\n    <summary>  When using the K-nearest neighbors method, one often ignores uncertainty in\nthe choice of K. To account for such uncertainty, Holmes and Adams (2002)\nproposed a Bayesian framework for K-nearest neighbors (KNN). Their Bayesian KNN\n(BKNN) approach uses a pseudo-likelihood function, and standard Markov chain\nMonte Carlo (MCMC) techniques to draw posterior samples. Holmes and Adams\n(2002) focused on the performance of BKNN in terms of misclassification error\nbut did not assess its ability to quantify uncertainty. We present some\nevidence to show that BKNN still significantly underestimates model\nuncertainty.\n</summary>\n    <author>\n      <name>Wanhua Su</name>\n    </author>\n    <author>\n      <name>Hugh Chipman</name>\n    </author>\n    <author>\n      <name>Mu Zhu</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0804.1325v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0804.1325v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0804.2848v1</id>\n    <updated>2008-04-17T16:25:48Z</updated>\n    <published>2008-04-17T16:25:48Z</published>\n    <title>Information Preserving Component Analysis: Data Projections for Flow\n  Cytometry Analysis</title>\n    <summary>  Flow cytometry is often used to characterize the malignant cells in leukemia\nand lymphoma patients, traced to the level of the individual cell. Typically,\nflow cytometric data analysis is performed through a series of 2-dimensional\nprojections onto the axes of the data set. Through the years, clinicians have\ndetermined combinations of different fluorescent markers which generate\nrelatively known expression patterns for specific subtypes of leukemia and\nlymphoma -- cancers of the hematopoietic system. By only viewing a series of\n2-dimensional projections, the high-dimensional nature of the data is rarely\nexploited. In this paper we present a means of determining a low-dimensional\nprojection which maintains the high-dimensional relationships (i.e.\ninformation) between differing oncological data sets. By using machine learning\ntechniques, we allow clinicians to visualize data in a low dimension defined by\na linear combination of all of the available markers, rather than just 2 at a\ntime. This provides an aid in diagnosing similar forms of cancer, as well as a\nmeans for variable selection in exploratory flow cytometric research. We refer\nto our method as Information Preserving Component Analysis (IPCA).\n</summary>\n    <author>\n      <name>Kevin M. Carter</name>\n    </author>\n    <author>\n      <name>Raviv Raich</name>\n    </author>\n    <author>\n      <name>William G. Finn</name>\n    </author>\n    <author>\n      <name>Alfred O. Hero III</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/JSTSP.2008.2011112</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/JSTSP.2008.2011112\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">26 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0804.2848v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0804.2848v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0805.1390v1</id>\n    <updated>2008-05-09T17:52:42Z</updated>\n    <published>2008-05-09T17:52:42Z</published>\n    <title>Random projection trees for vector quantization</title>\n    <summary>  A simple and computationally efficient scheme for tree-structured vector\nquantization is presented. Unlike previous methods, its quantization error\ndepends only on the intrinsic dimension of the data distribution, rather than\nthe apparent dimension of the space in which the data happen to lie.\n</summary>\n    <author>\n      <name>Sanjoy Dasgupta</name>\n    </author>\n    <author>\n      <name>Yoav Freund</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0805.1390v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0805.1390v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0806.2646v1</id>\n    <updated>2008-06-16T19:54:49Z</updated>\n    <published>2008-06-16T19:54:49Z</published>\n    <title>Manifold Learning: The Price of Normalization</title>\n    <summary>  We analyze the performance of a class of manifold-learning algorithms that\nfind their output by minimizing a quadratic form under some normalization\nconstraints. This class consists of Locally Linear Embedding (LLE), Laplacian\nEigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and\nDiffusion maps. We present and prove conditions on the manifold that are\nnecessary for the success of the algorithms. Both the finite sample case and\nthe limit case are analyzed. We show that there are simple manifolds in which\nthe necessary conditions are violated, and hence the algorithms cannot recover\nthe underlying manifolds. Finally, we present numerical results that\ndemonstrate our claims.\n</summary>\n    <author>\n      <name>Y. Goldberg</name>\n    </author>\n    <author>\n      <name>A. Zakai</name>\n    </author>\n    <author>\n      <name>D. Kushnir</name>\n    </author>\n    <author>\n      <name>Y. Ritov</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to JMLR</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0806.2646v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.2646v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0806.2669v1</id>\n    <updated>2008-06-16T20:41:57Z</updated>\n    <published>2008-06-16T20:41:57Z</published>\n    <title>Local Procrustes for Manifold Embedding: A Measure of Embedding Quality\n  and Embedding Algorithms</title>\n    <summary>  We present the Procrustes measure, a novel measure based on Procrustes\nrotation that enables quantitative comparison of the output of manifold-based\nembedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum\net al, 2000)). The measure also serves as a natural tool when choosing\ndimension-reduction parameters. We also present two novel dimension-reduction\ntechniques that attempt to minimize the suggested measure, and compare the\nresults of these techniques to the results of existing algorithms. Finally, we\nsuggest a simple iterative method that can be used to improve the output of\nexisting algorithms.\n</summary>\n    <author>\n      <name>Y. Goldberg</name>\n    </author>\n    <author>\n      <name>Y. Ritov</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to Journal of Machine Learning</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0806.2669v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.2669v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0806.2831v1</id>\n    <updated>2008-06-17T16:20:35Z</updated>\n    <published>2008-06-17T16:20:35Z</published>\n    <title>Supervised functional classification: A theoretical remark and some\n  comparisons</title>\n    <summary>  The problem of supervised classification (or discrimination) with functional\ndata is considered, with a special interest on the popular k-nearest neighbors\n(k-NN) classifier. First, relying on a recent result by Cerou and Guyader\n(2006), we prove the consistency of the k-NN classifier for functional data\nwhose distribution belongs to a broad family of Gaussian processes with\ntriangular covariance functions. Second, on a more practical side, we check the\nbehavior of the k-NN method when compared with a few other functional\nclassifiers. This is carried out through a small simulation study and the\nanalysis of several real functional data sets. While no global \"uniform\" winner\nemerges from such comparisons, the overall performance of the k-NN method,\ntogether with its sound intuitive motivation and relative simplicity, suggests\nthat it could represent a reasonable benchmark for the classification problem\nwith functional data.\n</summary>\n    <author>\n      <name>Amparo Baillo</name>\n    </author>\n    <author>\n      <name>Antonio Cuevas</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0806.2831v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.2831v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G07\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0806.4115v4</id>\n    <updated>2009-11-18T09:27:00Z</updated>\n    <published>2008-06-25T14:54:21Z</published>\n    <title>High-dimensional additive modeling</title>\n    <summary>  We propose a new sparsity-smoothness penalty for high-dimensional generalized\nadditive models. The combination of sparsity and smoothness is crucial for\nmathematical theory as well as performance for finite-sample data. We present a\ncomputationally efficient algorithm, with provable numerical convergence\nproperties, for optimizing the penalized likelihood. Furthermore, we provide\noracle results which yield asymptotic optimality of our estimator for high\ndimensional but sparse additive models. Finally, an adaptive version of our\nsparsity-smoothness penalized approach yields large additional performance\ngains.\n</summary>\n    <author>\n      <name>Lukas Meier</name>\n    </author>\n    <author>\n      <name>Sara van de Geer</name>\n    </author>\n    <author>\n      <name>Peter B\u00fchlmann</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1214/09-AOS692</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1214/09-AOS692\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in at http://dx.doi.org/10.1214/09-AOS692 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Annals of Statistics 2009, Vol. 37, No. 6B, 3779-3821</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0806.4115v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0806.4115v4\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"62G08, 62F12 (Primary), 62J07 (Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0808.0780v1</id>\n    <updated>2008-08-06T06:25:52Z</updated>\n    <published>2008-08-06T06:25:52Z</published>\n    <title>LLE with low-dimensional neighborhood representation</title>\n    <summary>  The local linear embedding algorithm (LLE) is a non-linear dimension-reducing\ntechnique, widely used due to its computational simplicity and intuitive\napproach. LLE first linearly reconstructs each input point from its nearest\nneighbors and then preserves these neighborhood relations in the\nlow-dimensional embedding. We show that the reconstruction weights computed by\nLLE capture the high-dimensional structure of the neighborhoods, and not the\nlow-dimensional manifold structure. Consequently, the weight vectors are highly\nsensitive to noise. Moreover, this causes LLE to converge to a linear\nprojection of the input, as opposed to its non-linear embedding goal. To\novercome both of these problems, we propose to compute the weight vectors using\na low-dimensional neighborhood representation. We prove theoretically that this\nstraightforward and computationally simple modification of LLE reduces LLE's\nsensitivity to noise. This modification also removes the need for\nregularization when the number of neighbors is larger than the dimension of the\ninput. We present numerical examples demonstrating both the perturbation and\nlinear projection problems, and the improved outputs using the low-dimensional\nneighborhood representation.\n</summary>\n    <author>\n      <name>Yair Goldberg</name>\n    </author>\n    <author>\n      <name>Ya'acov Ritov</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0808.0780v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0808.0780v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0808.2241v1</id>\n    <updated>2008-08-16T08:16:11Z</updated>\n    <published>2008-08-16T08:16:11Z</published>\n    <title>Persistent Clustering and a Theorem of J. Kleinberg</title>\n    <summary>  We construct a framework for studying clustering algorithms, which includes\ntwo key ideas: persistence and functoriality. The first encodes the idea that\nthe output of a clustering scheme should carry a multiresolution structure, the\nsecond the idea that one should be able to compare the results of clustering\nalgorithms as one varies the data set, for example by adding points or by\napplying functions to it. We show that within this framework, one can prove a\ntheorem analogous to one of J. Kleinberg, in which one obtains an existence and\nuniqueness theorem instead of a non-existence result. We explore further\nproperties of this unique scheme, stability and convergence are established.\n</summary>\n    <author>\n      <name>Gunnar Carlsson</name>\n    </author>\n    <author>\n      <name>Facundo Memoli</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0808.2241v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0808.2241v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0808.2337v1</id>\n    <updated>2008-08-18T19:07:06Z</updated>\n    <published>2008-08-18T19:07:06Z</published>\n    <title>Decomposable Principal Component Analysis</title>\n    <summary>  We consider principal component analysis (PCA) in decomposable Gaussian\ngraphical models. We exploit the prior information in these models in order to\ndistribute its computation. For this purpose, we reformulate the problem in the\nsparse inverse covariance (concentration) domain and solve the global\neigenvalue problem using a sequence of local eigenvalue problems in each of the\ncliques of the decomposable graph. We demonstrate the application of our\nmethodology in the context of decentralized anomaly detection in the Abilene\nbackbone network. Based on the topology of the network, we propose an\napproximate statistical graphical model and distribute the computation of PCA.\n</summary>\n    <author>\n      <name>Ami Wiesel</name>\n    </author>\n    <author>\n      <name>Alfred O. Hero III</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/TSP.2009.2025806</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/TSP.2009.2025806\" rel=\"related\"/>\n    <link href=\"http://arxiv.org/abs/0808.2337v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0808.2337v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0810.0901v2</id>\n    <updated>2010-08-12T20:29:23Z</updated>\n    <published>2008-10-06T07:59:45Z</published>\n    <title>Large Scale Variational Inference and Experimental Design for Sparse\n  Generalized Linear Models</title>\n    <summary>  Many problems of low-level computer vision and image processing, such as\ndenoising, deconvolution, tomographic reconstruction or super-resolution, can\nbe addressed by maximizing the posterior distribution of a sparse linear model\n(SLM). We show how higher-order Bayesian decision-making problems, such as\noptimizing image acquisition in magnetic resonance scanners, can be addressed\nby querying the SLM posterior covariance, unrelated to the density's mode. We\npropose a scalable algorithmic framework, with which SLM posteriors over full,\nhigh-resolution images can be approximated for the first time, solving a\nvariational optimization problem which is convex iff posterior mode finding is\nconvex. These methods successfully drive the optimization of sampling\ntrajectories for real-world magnetic resonance imaging through Bayesian\nexperimental design, which has not been attempted before. Our methodology\nprovides new insight into similarities and differences between sparse\nreconstruction and approximate Bayesian inference, and has important\nimplications for compressive sensing of real-world images.\n</summary>\n    <author>\n      <name>Matthias W. Seeger</name>\n    </author>\n    <author>\n      <name>Hannes Nickisch</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">34 pages, 6 figures, technical report (submitted)</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0810.0901v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0810.0901v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0810.4553v1</id>\n    <updated>2008-10-24T21:05:16Z</updated>\n    <published>2008-10-24T21:05:16Z</published>\n    <title>Online Coordinate Boosting</title>\n    <summary>  We present a new online boosting algorithm for adapting the weights of a\nboosted classifier, which yields a closer approximation to Freund and\nSchapire's AdaBoost algorithm than previous online boosting algorithms. We also\ncontribute a new way of deriving the online algorithm that ties together\nprevious online boosting work. We assume that the weak hypotheses were selected\nbeforehand, and only their weights are updated during online boosting. The\nupdate rule is derived by minimizing AdaBoost's loss when viewed in an\nincremental form. The equations show that optimization is computationally\nexpensive. However, a fast online approximation is possible. We compare\napproximation error to batch AdaBoost on synthetic datasets and generalization\nerror on face datasets and the MNIST dataset.\n</summary>\n    <author>\n      <name>Raphael Pelossof</name>\n    </author>\n    <author>\n      <name>Michael Jones</name>\n    </author>\n    <author>\n      <name>Ilia Vovsha</name>\n    </author>\n    <author>\n      <name>Cynthia Rudin</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">9 pages, 4 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0810.4553v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0810.4553v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0810.5117v1</id>\n    <updated>2008-10-28T19:42:15Z</updated>\n    <published>2008-10-28T19:42:15Z</published>\n    <title>A non-negative expansion for small Jensen-Shannon Divergences</title>\n    <summary>  In this report, we derive a non-negative series expansion for the\nJensen-Shannon divergence (JSD) between two probability distributions. This\nseries expansion is shown to be useful for numerical calculations of the JSD,\nwhen the probability distributions are nearly equal, and for which,\nconsequently, small numerical errors dominate evaluation.\n</summary>\n    <author>\n      <name>Anil Raj</name>\n    </author>\n    <author>\n      <name>Chris H. Wiggins</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">4 page technical report, 2 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0810.5117v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0810.5117v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0811.1239v1</id>\n    <updated>2008-11-07T23:33:40Z</updated>\n    <published>2008-11-07T23:33:40Z</published>\n    <title>Improved Estimation of High-dimensional Ising Models</title>\n    <summary>  We consider the problem of jointly estimating the parameters as well as the\nstructure of binary valued Markov Random Fields, in contrast to earlier work\nthat focus on one of the two problems. We formulate the problem as a\nmaximization of $\\ell_1$-regularized surrogate likelihood that allows us to\nfind a sparse solution. Our optimization technique efficiently incorporates the\ncutting-plane algorithm in order to obtain a tighter outer bound on the\nmarginal polytope, which results in improvement of both parameter estimates and\napproximation to marginals. On synthetic data, we compare our algorithm on the\ntwo estimation tasks to the other existing methods. We analyze the method in\nthe high-dimensional setting, where the number of dimensions $p$ is allowed to\ngrow with the number of observations $n$. The rate of convergence of the\nestimate is demonstrated to depend explicitly on the sparsity of the underlying\ngraph.\n</summary>\n    <author>\n      <name>M. Kolar</name>\n    </author>\n    <author>\n      <name>E. P. Xing</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">8 pages, 3 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0811.1239v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.1239v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0811.3499v1</id>\n    <updated>2008-11-21T10:17:00Z</updated>\n    <published>2008-11-21T10:17:00Z</published>\n    <title>Kernel Regression by Mode Calculation of the Conditional Probability\n  Distribution</title>\n    <summary>  The most direct way to express arbitrary dependencies in datasets is to\nestimate the joint distribution and to apply afterwards the argmax-function to\nobtain the mode of the corresponding conditional distribution. This method is\nin practice difficult, because it requires a global optimization of a\ncomplicated function, the joint distribution by fixed input variables. This\narticle proposes a method for finding global maxima if the joint distribution\nis modeled by a kernel density estimation. Some experiments show advantages and\nshortcomings of the resulting regression method in comparison to the standard\nNadaraya-Watson regression technique, which approximates the optimum by the\nexpectation value.\n</summary>\n    <author>\n      <name>Steffen Kuehn</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 5 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0811.3499v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.3499v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0811.3579v3</id>\n    <updated>2009-07-22T22:17:26Z</updated>\n    <published>2008-11-21T16:47:58Z</published>\n    <title>Entropy inference and the James-Stein estimator, with application to\n  nonlinear gene association networks</title>\n    <summary>  We present a procedure for effective estimation of entropy and mutual\ninformation from small-sample data, and apply it to the problem of inferring\nhigh-dimensional gene association networks. Specifically, we develop a\nJames-Stein-type shrinkage estimator, resulting in a procedure that is highly\nefficient statistically as well as computationally. Despite its simplicity, we\nshow that it outperforms eight other entropy estimation procedures across a\ndiverse range of sampling scenarios and data-generating models, even in cases\nof severe undersampling. We illustrate the approach by analyzing E. coli gene\nexpression data and computing an entropy-based gene-association network from\ngene expression data. A computer program is available that implements the\nproposed shrinkage estimator.\n</summary>\n    <author>\n      <name>Jean Hausser</name>\n    </author>\n    <author>\n      <name>Korbinian Strimmer</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, 3 figures, 1 table</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research 10: 1469-1484 (2009)</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0811.3579v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.3579v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0811.3619v1</id>\n    <updated>2008-11-21T19:45:15Z</updated>\n    <published>2008-11-21T19:45:15Z</published>\n    <title>Random Forests: some methodological insights</title>\n    <summary>  This paper examines from an experimental perspective random forests, the\nincreasingly used statistical method for classification and regression problems\nintroduced by Leo Breiman in 2001. It first aims at confirming, known but\nsparse, advice for using random forests and at proposing some complementary\nremarks for both standard problems as well as high dimensional ones for which\nthe number of variables hugely exceeds the sample size. But the main\ncontribution of this paper is twofold: to provide some insights about the\nbehavior of the variable importance index based on random forests and in\naddition, to propose to investigate two classical issues of variable selection.\nThe first one is to find important variables for interpretation and the second\none is more restrictive and try to design a good prediction model. The strategy\ninvolves a ranking of explanatory variables using the random forests score of\nimportance and a stepwise ascending variable introduction strategy.\n</summary>\n    <author>\n      <name>Robin Genuer</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LM-Orsay</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Jean-Michel Poggi</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">LM-Orsay</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Christine Tuleau</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">JAD</arxiv:affiliation>\n    </author>\n    <link href=\"http://arxiv.org/abs/0811.3619v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.3619v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0811.4208v1</id>\n    <updated>2008-11-26T14:56:26Z</updated>\n    <published>2008-11-26T14:56:26Z</published>\n    <title>An information-theoretic derivation of min-cut based clustering</title>\n    <summary>  Min-cut clustering, based on minimizing one of two heuristic cost-functions\nproposed by Shi and Malik, has spawned tremendous research, both analytic and\nalgorithmic, in the graph partitioning and image segmentation communities over\nthe last decade. It is however unclear if these heuristics can be derived from\na more general principle facilitating generalization to new problem settings.\nMotivated by an existing graph partitioning framework, we derive relationships\nbetween optimizing relevance information, as defined in the Information\nBottleneck method, and the regularized cut in a K-partitioned graph. For fast\nmixing graphs, we show that the cost functions introduced by Shi and Malik can\nbe well approximated as the rate of loss of predictive information about the\nlocation of random walkers on the graph. For graphs generated from a stochastic\nalgorithm designed to model community structure, the optimal information\ntheoretic partition and the optimal min-cut partition are shown to be the same\nwith high probability.\n</summary>\n    <author>\n      <name>Anil Raj</name>\n    </author>\n    <author>\n      <name>Chris H. Wiggins</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 3 figures, two-column, submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0811.4208v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0811.4208v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0901.0026v1</id>\n    <updated>2008-12-30T23:09:18Z</updated>\n    <published>2008-12-30T23:09:18Z</published>\n    <title>On the Geometry of Discrete Exponential Families with Application to\n  Exponential Random Graph Models</title>\n    <summary>  There has been an explosion of interest in statistical models for analyzing\nnetwork data, and considerable interest in the class of exponential random\ngraph (ERG) models, especially in connection with difficulties in computing\nmaximum likelihood estimates. The issues associated with these difficulties\nrelate to the broader structure of discrete exponential families. This paper\nre-examines the issues in two parts. First we consider the closure of\n$k$-dimensional exponential families of distribution with discrete base measure\nand polyhedral convex support $\\mathrm{P}$. We show that the normal fan of\n$\\mathrm{P}$ is a geometric object that plays a fundamental role in deriving\nthe statistical and geometric properties of the corresponding extended\nexponential families. We discuss its relevance to maximum likelihood\nestimation, both from a theoretical and computational standpoint. Second, we\napply our results to the analysis of ERG models. In particular, by means of a\ndetailed example, we provide some characterization of the properties of ERG\nmodels, and, in particular, of certain behaviors of ERG models known as\ndegeneracy.\n</summary>\n    <author>\n      <name>Stephen E. Fienberg</name>\n    </author>\n    <author>\n      <name>Alessandro Rinaldo</name>\n    </author>\n    <author>\n      <name>Yi Zhou</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0901.0026v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0901.0026v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0903.0649v1</id>\n    <updated>2009-03-03T22:55:18Z</updated>\n    <published>2009-03-03T22:55:18Z</published>\n    <title>The Nonparanormal: Semiparametric Estimation of High Dimensional\n  Undirected Graphs</title>\n    <summary>  Recent methods for estimating sparse undirected graphs for real-valued data\nin high dimensional problems rely heavily on the assumption of normality. We\nshow how to use a semiparametric Gaussian copula--or \"nonparanormal\"--for high\ndimensional inference. Just as additive models extend linear models by\nreplacing linear functions with a set of one-dimensional smooth functions, the\nnonparanormal extends the normal by transforming the variables by smooth\nfunctions. We derive a method for estimating the nonparanormal, study the\nmethod's theoretical properties, and show that it works well in many examples.\n</summary>\n    <author>\n      <name>Han Liu</name>\n    </author>\n    <author>\n      <name>John Lafferty</name>\n    </author>\n    <author>\n      <name>Larry Wasserman</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0903.0649v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0903.0649v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0904.0838v2</id>\n    <updated>2011-04-07T09:01:41Z</updated>\n    <published>2009-04-06T03:36:01Z</published>\n    <title>Finding Exogenous Variables in Data with Many More Variables than\n  Observations</title>\n    <summary>  Many statistical methods have been proposed to estimate causal models in\nclassical situations with fewer variables than observations (p&lt;n, p: the number\nof variables and n: the number of observations). However, modern datasets\nincluding gene expression data need high-dimensional causal modeling in\nchallenging situations with orders of magnitude more variables than\nobservations (p&gt;&gt;n). In this paper, we propose a method to find exogenous\nvariables in a linear non-Gaussian causal model, which requires much smaller\nsample sizes than conventional methods and works even when p&gt;&gt;n. The key idea\nis to identify which variables are exogenous based on non-Gaussianity instead\nof estimating the entire structure of the model. Exogenous variables work as\ntriggers that activate a causal chain in the model, and their identification\nleads to more efficient experimental designs and better understanding of the\ncausal mechanism. We present experiments with artificial data and real-world\ngene expression data to evaluate the method.\n</summary>\n    <author>\n      <name>Shohei Shimizu</name>\n    </author>\n    <author>\n      <name>Takashi Washio</name>\n    </author>\n    <author>\n      <name>Aapo Hyvarinen</name>\n    </author>\n    <author>\n      <name>Seiya Imoto</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/978-3-642-15819-3_10</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/978-3-642-15819-3_10\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">A revised version of this was published in Proc. ICANN2010</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ARTIFICIAL NEURAL NETWORKS - ICANN 2010. Lecture Notes in Computer\n  Science, 2010, Volume 6352/2010, 67-76</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0904.0838v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.0838v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0904.3523v3</id>\n    <updated>2010-05-31T09:05:18Z</updated>\n    <published>2009-04-22T18:38:07Z</published>\n    <title>Structured Variable Selection with Sparsity-Inducing Norms</title>\n    <summary>  We consider the empirical risk minimization problem for linear supervised\nlearning, with regularization by structured sparsity-inducing norms. These are\ndefined as sums of Euclidean norms on certain subsets of variables, extending\nthe usual $\\ell_1$-norm and the group $\\ell_1$-norm by allowing the subsets to\noverlap. This leads to a specific set of allowed nonzero patterns for the\nsolutions of such problems. We first explore the relationship between the\ngroups defining the norm and the resulting nonzero patterns, providing both\nforward and backward algorithms to go back and forth from groups to patterns.\nThis allows the design of norms adapted to specific prior knowledge expressed\nin terms of nonzero patterns. We also present an efficient active set\nalgorithm, and analyze the consistency of variable selection for least-squares\nlinear regression in low and high-dimensional settings.\n</summary>\n    <author>\n      <name>Rodolphe Jenatton</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Rocquencourt</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Jean-Yves Audibert</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Rocquencourt</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Francis Bach</name>\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">INRIA Rocquencourt</arxiv:affiliation>\n    </author>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Journal of Machine Learning Research 12 (2011) 2777-2824</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0904.3523v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0904.3523v3\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0905.1540v1</id>\n    <updated>2009-05-11T04:37:02Z</updated>\n    <published>2009-05-11T04:37:02Z</published>\n    <title>Supplementary material for Markov equivalence for ancestral graphs</title>\n    <summary>  We prove that the criterion for Markov equivalence provided by Zhao et al.\n(2005) may involve a set of features of a graph that is exponential in the\nnumber of vertices.\n</summary>\n    <author>\n      <name>R. A. Ali</name>\n    </author>\n    <author>\n      <name>T. Richardson</name>\n    </author>\n    <author>\n      <name>P. Spirtes</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">2 pages, 1 figure, supplement to paper to appear in the Annals of\n  Statistics</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0905.1540v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0905.1540v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0905.2138v1</id>\n    <updated>2009-05-13T16:04:02Z</updated>\n    <published>2009-05-13T16:04:02Z</published>\n    <title>A more robust boosting algorithm</title>\n    <summary>  We present a new boosting algorithm, motivated by the large margins theory\nfor boosting. We give experimental evidence that the new algorithm is\nsignificantly more robust against label noise than existing boosting algorithm.\n</summary>\n    <author>\n      <name>Yoav Freund</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0905.2138v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0905.2138v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0906.3590v1</id>\n    <updated>2009-06-19T07:29:34Z</updated>\n    <published>2009-06-19T07:29:34Z</published>\n    <title>Forest Garrote</title>\n    <summary>  Variable selection for high-dimensional linear models has received a lot of\nattention lately, mostly in the context of l1-regularization. Part of the\nattraction is the variable selection effect: parsimonious models are obtained,\nwhich are very suitable for interpretation. In terms of predictive power,\nhowever, these regularized linear models are often slightly inferior to machine\nlearning procedures like tree ensembles. Tree ensembles, on the other hand,\nlack usually a formal way of variable selection and are difficult to visualize.\nA Garrote-style convex penalty for trees ensembles, in particular Random\nForests, is proposed. The penalty selects functional groups of nodes in the\ntrees. These could be as simple as monotone functions of individual predictor\nvariables. This yields a parsimonious function fit, which lends itself easily\nto visualization and interpretation. The predictive power is maintained at\nleast at the same level as the original tree ensemble. A key feature of the\nmethod is that, once a tree ensemble is fitted, no further tuning parameter\nneeds to be selected. The empirical performance is demonstrated on a wide array\nof datasets.\n</summary>\n    <author>\n      <name>Nicolai Meinshausen</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 pages, 3 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0906.3590v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.3590v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0906.4258v1</id>\n    <updated>2009-06-23T13:45:10Z</updated>\n    <published>2009-06-23T13:45:10Z</published>\n    <title>The Feature Importance Ranking Measure</title>\n    <summary>  Most accurate predictions are typically obtained by learning machines with\ncomplex feature spaces (as e.g. induced by kernels). Unfortunately, such\ndecision rules are hardly accessible to humans and cannot easily be used to\ngain insights about the application domain. Therefore, one often resorts to\nlinear models in combination with variable selection, thereby sacrificing some\npredictive power for presumptive interpretability. Here, we introduce the\nFeature Importance Ranking Measure (FIRM), which by retrospective analysis of\narbitrary learning machines allows to achieve both excellent predictive\nperformance and superior interpretation. In contrast to standard raw feature\nweighting, FIRM takes the underlying correlation structure of the features into\naccount. Thereby, it is able to discover the most relevant features, even if\ntheir appearance in the training data is entirely prevented by noise. The\ndesirable properties of FIRM are investigated analytically and illustrated in\nsimulations.\n</summary>\n    <author>\n      <name>Alexander Zien</name>\n    </author>\n    <author>\n      <name>Nicole Kraemer</name>\n    </author>\n    <author>\n      <name>Soeren Sonnenburg</name>\n    </author>\n    <author>\n      <name>Gunnar Raetsch</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">15 pages, 3 figures. to appear in the Proceedings of the European\n  Conference on Machine Learning and Principles and Practice of Knowledge\n  Discovery in Databases (ECML/PKDD), 2009</arxiv:comment>\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Proceedings of the European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD),\n  Lecture Notes in Computer Science 5782, 694 - 709, 2009</arxiv:journal_ref>\n    <link href=\"http://arxiv.org/abs/0906.4258v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.4258v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0906.4391v1</id>\n    <updated>2009-06-24T02:17:36Z</updated>\n    <published>2009-06-24T02:17:36Z</published>\n    <title>KNIFE: Kernel Iterative Feature Extraction</title>\n    <summary>  Selecting important features in non-linear or kernel spaces is a difficult\nchallenge in both classification and regression problems. When many of the\nfeatures are irrelevant, kernel methods such as the support vector machine and\nkernel ridge regression can sometimes perform poorly. We propose weighting the\nfeatures within a kernel with a sparse set of weights that are estimated in\nconjunction with the original classification or regression problem. The\niterative algorithm, KNIFE, alternates between finding the coefficients of the\noriginal problem and finding the feature weights through kernel linearization.\nIn addition, a slight modification of KNIFE yields an efficient algorithm for\nfinding feature regularization paths, or the paths of each feature's weight.\nSimulation results demonstrate the utility of KNIFE for both kernel regression\nand support vector machines with a variety of kernels. Feature path\nrealizations also reveal important non-linear correlations among features that\nprove useful in determining a subset of significant variables. Results on vowel\nrecognition data, Parkinson's disease data, and microarray data are also given.\n</summary>\n    <author>\n      <name>Genevera I. Allen</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0906.4391v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0906.4391v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.0781v1</id>\n    <updated>2009-07-04T18:24:06Z</updated>\n    <published>2009-07-04T18:24:06Z</published>\n    <title>Bayesian Agglomerative Clustering with Coalescents</title>\n    <summary>  We introduce a new Bayesian model for hierarchical clustering based on a\nprior over trees called Kingman's coalescent. We develop novel greedy and\nsequential Monte Carlo inferences which operate in a bottom-up agglomerative\nfashion. We show experimentally the superiority of our algorithms over others,\nand demonstrate our approach in document clustering and phylolinguistics.\n</summary>\n    <author>\n      <name>Yee Whye Teh</name>\n    </author>\n    <author>\n      <name>Hal Daum\u00e9 III</name>\n    </author>\n    <author>\n      <name>Daniel Roy</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NIPS 2008</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.0781v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.0781v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.1013v1</id>\n    <updated>2009-07-06T15:29:00Z</updated>\n    <published>2009-07-06T15:29:00Z</published>\n    <title>Visualizing Topics with Multi-Word Expressions</title>\n    <summary>  We describe a new method for visualizing topics, the distributions over terms\nthat are automatically extracted from large text corpora using latent variable\nmodels. Our method finds significant $n$-grams related to a topic, which are\nthen used to help understand and interpret the underlying distribution.\nCompared with the usual visualization, which simply lists the most probable\ntopical terms, the multi-word expressions provide a better intuitive impression\nfor what a topic is \"about.\" Our approach is based on a language model of\narbitrary length expressions, for which we develop a new methodology based on\nnested permutation tests to find significant phrases. We show that this method\noutperforms the more standard use of $\\chi^2$ and likelihood ratio tests. We\nillustrate the topic presentations on corpora of scientific abstracts and news\narticles.\n</summary>\n    <author>\n      <name>David M. Blei</name>\n    </author>\n    <author>\n      <name>John D. Lafferty</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0907.1013v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.1013v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.2337v2</id>\n    <updated>2013-04-02T14:30:31Z</updated>\n    <published>2009-07-14T12:09:33Z</published>\n    <title>Sparsistent Estimation of Time-Varying Discrete Markov Random Fields</title>\n    <summary>  Network models have been popular for modeling and representing complex\nrelationships and dependencies between observed variables. When data comes from\na dynamic stochastic process, a single static network model cannot adequately\ncapture transient dependencies, such as, gene regulatory dependencies\nthroughout a developmental cycle of an organism. Kolar et al (2010b) proposed a\nmethod based on kernel-smoothing l1-penalized logistic regression for\nestimating time-varying networks from nodal observations collected from a\ntime-series of observational data. In this paper, we establish conditions under\nwhich the proposed method consistently recovers the structure of a time-varying\nnetwork. This work complements previous empirical findings by providing sound\ntheoretical guarantees for the proposed estimation procedure. For completeness,\nwe include numerical simulations in the paper.\n</summary>\n    <author>\n      <name>Mladen Kolar</name>\n    </author>\n    <author>\n      <name>Eric P. Xing</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Updated references. Reorganized proofs. Added simulation studies</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.2337v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.2337v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.3740v1</id>\n    <updated>2009-07-21T20:21:38Z</updated>\n    <published>2009-07-21T20:21:38Z</published>\n    <title>Empirical Bernstein Bounds and Sample Variance Penalization</title>\n    <summary>  We give improved constants for data dependent and variance sensitive\nconfidence bounds, called empirical Bernstein bounds, and extend these\ninequalities to hold uniformly over classes of functionswhose growth function\nis polynomial in the sample size n. The bounds lead us to consider sample\nvariance penalization, a novel learning method which takes into account the\nempirical variance of the loss function. We give conditions under which sample\nvariance penalization is effective. In particular, we present a bound on the\nexcess risk incurred by the method. Using this, we argue that there are\nsituations in which the excess risk of our method is of order 1/n, while the\nexcess risk of empirical risk minimization is of order 1/sqrt/{n}. We show some\nexperimental results, which confirm the theory. Finally, we discuss the\npotential application of our results to sample compression schemes.\n</summary>\n    <author>\n      <name>Andreas Maurer</name>\n    </author>\n    <author>\n      <name>Massimiliano Pontil</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10 pages, 1 figure, Proc. Computational Learning Theory Conference\n  (COLT 2009)</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.3740v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.3740v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.4643v2</id>\n    <updated>2009-10-09T11:31:24Z</updated>\n    <published>2009-07-27T14:43:22Z</published>\n    <title>Mean-Field Theory of Meta-Learning</title>\n    <summary>  We discuss here the mean-field theory for a cellular automata model of\nmeta-learning. The meta-learning is the process of combining outcomes of\nindividual learning procedures in order to determine the final decision with\nhigher accuracy than any single learning method. Our method is constructed from\nan ensemble of interacting, learning agents, that acquire and process incoming\ninformation using various types, or different versions of machine learning\nalgorithms. The abstract learning space, where all agents are located, is\nconstructed here using a fully connected model that couples all agents with\nrandom strength values. The cellular automata network simulates the higher\nlevel integration of information acquired from the independent learning trials.\nThe final classification of incoming input data is therefore defined as the\nstationary state of the meta-learning system using simple majority rule, yet\nthe minority clusters that share opposite classification outcome can be\nobserved in the system. Therefore, the probability of selecting proper class\nfor a given input data, can be estimated even without the prior knowledge of\nits affiliation. The fuzzy logic can be easily introduced into the system, even\nif learning agents are build from simple binary classification machine learning\nalgorithms by calculating the percentage of agreeing agents.\n</summary>\n    <author>\n      <name>Dariusz Plewczynski</name>\n    </author>\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1088/1742-5468/2009/11/P11003</arxiv:doi>\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1088/1742-5468/2009/11/P11003\" rel=\"related\"/>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">23 pages</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0907.4643v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.4643v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0907.5494v1</id>\n    <updated>2009-07-31T09:19:49Z</updated>\n    <published>2009-07-31T09:19:49Z</published>\n    <title>How the initialization affects the stability of the k-means algorithm</title>\n    <summary>  We investigate the role of the initialization for the stability of the\nk-means clustering algorithm. As opposed to other papers, we consider the\nactual k-means algorithm and do not ignore its property of getting stuck in\nlocal optima. We are interested in the actual clustering, not only in the costs\nof the solution. We analyze when different initializations lead to the same\nlocal optimum, and when they lead to different local optima. This enables us to\nprove that it is reasonable to select the number of clusters based on stability\nscores.\n</summary>\n    <author>\n      <name>Sebastien Bubeck</name>\n    </author>\n    <author>\n      <name>Marina Meila</name>\n    </author>\n    <author>\n      <name>Ulrike von Luxburg</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/0907.5494v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0907.5494v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0908.2284v1</id>\n    <updated>2009-08-17T05:10:22Z</updated>\n    <published>2009-08-17T05:10:22Z</published>\n    <title>Classification by Set Cover: The Prototype Vector Machine</title>\n    <summary>  We introduce a new nearest-prototype classifier, the prototype vector machine\n(PVM). It arises from a combinatorial optimization problem which we cast as a\nvariant of the set cover problem. We propose two algorithms for approximating\nits solution. The PVM selects a relatively small number of representative\npoints which can then be used for classification. It contains 1-NN as a special\ncase. The method is compatible with any dissimilarity measure, making it\namenable to situations in which the data are not embedded in an underlying\nfeature space or in which using a non-Euclidean metric is desirable. Indeed, we\ndemonstrate on the much studied ZIP code data how the PVM can reap the benefits\nof a problem-specific metric. In this example, the PVM outperforms the highly\nsuccessful 1-NN with tangent distance, and does so retaining fewer than half of\nthe data points. This example highlights the strengths of the PVM in yielding a\nlow-error, highly interpretable model. Additionally, we apply the PVM to a\nprotein classification problem in which a kernel-based distance is used.\n</summary>\n    <author>\n      <name>Jacob Bien</name>\n    </author>\n    <author>\n      <name>Robert Tibshirani</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">24 pages, 11 figures</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0908.2284v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0908.2284v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0908.2579v2</id>\n    <updated>2009-10-29T19:40:49Z</updated>\n    <published>2009-08-18T14:35:03Z</published>\n    <title>Convex Multiview Fisher Discriminant Analysis</title>\n    <summary>  Section 1.3 was incorrect, and 2.1 will be removed from further submissions.\nA rewritten version will be posted in the future.\n</summary>\n    <author>\n      <name>Tom Diethe</name>\n    </author>\n    <author>\n      <name>John Shawe-Taylor</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">This paper has been withdrawn</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0908.2579v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0908.2579v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/0908.3321v1</id>\n    <updated>2009-08-23T17:45:07Z</updated>\n    <published>2009-08-23T17:45:07Z</published>\n    <title>Relative Expected Improvement in Kriging Based Optimization</title>\n    <summary>  We propose an extension of the concept of Expected Improvement criterion\ncommonly used in Kriging based optimization. We extend it for more complex\nKriging models, e.g. models using derivatives. The target field of application\nare CFD problems, where objective function are extremely expensive to evaluate,\nbut the theory can be also used in other fields.\n</summary>\n    <author>\n      <name>\u0141ukasz \u0141aniewski-Wo\u0142\u0142k</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">EUROGEN2009 Kwakow conference</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/0908.3321v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/0908.3321v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"